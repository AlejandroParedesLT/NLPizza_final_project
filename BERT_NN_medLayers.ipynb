{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2sMmlrTBL6W"
   },
   "source": [
    "# Alejandro Paredes, Parameter tuning of BERT\n",
    "\n",
    "https://arunm8489.medium.com/understanding-distil-bert-in-depth-5f2ca92cf1ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pNpc6KmBRY4",
    "outputId": "a02a3279-1b48-48b8-da39-8402311f6642"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwAV24NJ-ZSj",
    "outputId": "ca33db64-8fcd-4613-f9d9-dba631f74faa"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets peft evaluate datasets contractions tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "C2gg1Syx-s44",
    "outputId": "eb798168-37c2-45d0-e18b-69982ec24a05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DistilBertModel,\n",
    "    DistilBertTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import preprocessor as p\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yrj-hSULAi_a",
    "outputId": "c2c054b0-7912-4250-f086-488b4b093400"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "        num_rows: 132046\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "        num_rows: 14672\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load all CSV files in the ./data directory\n",
    "#data_files = \"./data/*.csv\"\n",
    "\n",
    "# Load and combine the datasets\n",
    "dataset = load_dataset(\"csv\", data_files=\"./data/2017_1.csv\")#data_files)\n",
    "\n",
    "# Filter and split the dataset\n",
    "df  = dataset['train'].filter(\n",
    "    lambda example: example['headline'] is not None and example['headline'].strip() != ''\n",
    ").train_test_split(test_size=0.1)\n",
    "\n",
    "# Display the resulting dataset\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_8Wqw02VBL6f"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "#Define label maps\n",
    "id2label = {0:\"UNDEFINED\" ,1:\"LEFT\",2:\"RIGHT\",3:\"CENTER\"}\n",
    "label2id = {\"UNDEFINED\": 0, \"LEFT\": 1, \"RIGHT\": 2, \"CENTER\": 3}\n",
    "\n",
    "tokenizer =  DistilBertTokenizer.from_pretrained(model_checkpoint, add_prefix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r1fchlLlBL6g"
   },
   "outputs": [],
   "source": [
    "#lemmatization and removing stopwords\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "\n",
    "def preprocess(text):\n",
    "    def is_english_word(word):\n",
    "        \"\"\"Function to filter out non-English words.\"\"\"\n",
    "        return bool(re.match(r'^[a-zA-Z]+$', word))\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = p.clean(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKd0MpK9BFPv",
    "outputId": "82415701-c8f9-4104-a2e0-91c50eee79a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  Scott Walkerâ€™s School Bonus \n",
      "\n",
      "Tokenized Text:  ['scott', 'walker', 's', 'school', 'bonus'] \n",
      "\n",
      "Token IDs:  [3660, 5232, 1521, 1055, 2082, 6781]\n",
      "Original Text:  BRIEF-Deutsche Bank hires William White as head of US Life Sciences \n",
      "\n",
      "Tokenized Text:  ['brief', '-', 'deutsche', 'bank', 'hires', 'william', 'white', 'as', 'head', 'of', 'us', 'life', 'sciences'] \n",
      "\n",
      "Token IDs:  [4766, 1011, 11605, 2924, 28208, 2520, 2317, 2004, 2132, 1997, 2149, 2166, 4163]\n",
      "Original Text:  State police: 3 dead, including deputy, in rural Arkansas \n",
      "\n",
      "Tokenized Text:  ['state', 'police', ':', '3', 'dead', ',', 'including', 'deputy', ',', 'in', 'rural', 'arkansas'] \n",
      "\n",
      "Token IDs:  [2110, 2610, 1024, 1017, 2757, 1010, 2164, 4112, 1010, 1999, 3541, 6751]\n",
      "Original Text:  Republicans are incapable of crafting a humane health care bill \n",
      "\n",
      "Tokenized Text:  ['republicans', 'are', 'incapable', 'of', 'craft', '##ing', 'a', 'humane', 'health', 'care', 'bill'] \n",
      "\n",
      "Token IDs:  [10643, 2024, 19907, 1997, 7477, 2075, 1037, 23369, 2740, 2729, 3021]\n",
      "Original Text:  Election 2017: The south-east Scotland seat where every vote will count \n",
      "\n",
      "Tokenized Text:  ['election', '2017', ':', 'the', 'south', '-', 'east', 'scotland', 'seat', 'where', 'every', 'vote', 'will', 'count'] \n",
      "\n",
      "Token IDs:  [2602, 2418, 1024, 1996, 2148, 1011, 2264, 3885, 2835, 2073, 2296, 3789, 2097, 4175]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('Original Text: ', df['train']['headline'][i], '\\n')\n",
    "    print('Tokenized Text: ', tokenizer.tokenize(preprocess(df['train']['headline'][i])), '\\n')\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['headline'][i])))\n",
    "\n",
    "#for i in range(2):\n",
    "    #print('Original Text: ', df['train']['body'][i], '\\n')\n",
    "    #print('Tokenized Text: ', tokenizer.tokenize(preprocess(df['train']['body'][i])), '\\n')\n",
    "    #print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['body'][i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B99-PUpHBL6i",
    "outputId": "2236512c-c887-4f9c-bb2d-3bca43abec77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "40\n",
      "0\n",
      "15\n",
      "17700\n",
      "87445\n"
     ]
    }
   ],
   "source": [
    "texts = df['train']['headline']\n",
    "\n",
    "# Handle None or missing values by filtering out None entries\n",
    "text_lengths = [len(text.split(' ')) if text is not None else 0 for text in texts]\n",
    "\n",
    "print(min(text_lengths))\n",
    "print(max(text_lengths))\n",
    "\n",
    "# Count how many texts have 300 or more words\n",
    "print(sum([1 for length in text_lengths if length >= 300]))\n",
    "\n",
    "# Repeat for the 'body' column\n",
    "texts = df['train']['body']\n",
    "\n",
    "# Handle None or missing values by filtering out None entries\n",
    "text_lengths = [len(text.split()) if text is not None else 0 for text in texts]\n",
    "\n",
    "print(min(text_lengths))\n",
    "print(max(text_lengths))\n",
    "\n",
    "# Count how many texts have 300 or more words\n",
    "print(sum([1 for length in text_lengths if length >= 300]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWWgLAscBL6j"
   },
   "source": [
    "# **Creating a custom model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0vbHLyjBL6j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self, model_checkpoint='distilbert-base-uncased'):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(model_checkpoint, num_labels=8)\n",
    "\n",
    "        # Freeze DistilBERT parameters (except for the new layers)\n",
    "        for param in self.l1.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Add custom query layers for each transformer layer\n",
    "        # Custom query low-rank matrices for the attention mechanism (inspired by LoRA)\n",
    "        self.DocClass_A = torch.nn.Parameter(torch.randn(768, 768))  # Matrix A (query transformation)\n",
    "        self.DocClass_B = torch.nn.Parameter(torch.randn(768, 768))  # Matrix B (query transformation)\n",
    "\n",
    "        # Additional layers for classification\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.fc1 = torch.nn.Linear(768, 1024)  \n",
    "        self.classifier = torch.nn.Linear(1024, 5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def add_query_layers(self):\n",
    "        # Access the attention layers of DistilBERT\n",
    "        for i, layer in enumerate(self.l1.transformer.layer):\n",
    "            # Access query (q_lin) and create new layers\n",
    "            # These layers will be trainable while keeping the base layers frozen\n",
    "            q_lin = layer.attention.q_lin\n",
    "            \n",
    "            # Freezing the original query layer weights, and adding LoRA layers\n",
    "            q_lin.base_layer.weight.requires_grad = False\n",
    "            q_lin.base_layer.bias.requires_grad = False\n",
    "            \n",
    "            # Add new LoRA layers (A and B matrices)\n",
    "            q_DocClass_w = torch.nn.Parameter(torch.randn_like(q_lin.base_layer.weight))\n",
    "            q_DocClass_b = torch.nn.Parameter(torch.randn_like(q_lin.base_layer.bias))\n",
    "            \n",
    "            # Register new parameters to the model\n",
    "            self.register_parameter(f'layer_{i}_lora_A', q_DocClass_w)\n",
    "            self.register_parameter(f'layer_{i}_lora_B', q_DocClass_b)\n",
    "            \n",
    "            # Re-define q_lin as a combination of the base layer and LoRA layers\n",
    "            q_lin.DocClass_w = q_DocClass_w\n",
    "            q_lin.DocClass_b = q_DocClass_b\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        pooler = self.fc1(pooler)\n",
    "        pooler = self.relu(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        output = self.softmax(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjBjQXj4BL6j",
    "outputId": "5e40bd38-b4d1-4dc0-e6fc-5527e3faea3d"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'base_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      8\u001b[0m LEARNING_RATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-05\u001b[39m\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDistillBERTClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Creating the loss function and optimizer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36mDistillBERTClass.__init__\u001b[1;34m(self, model_checkpoint)\u001b[0m\n\u001b[0;32m     11\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Add custom query layers for each transformer layer\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_query_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Add intermediate layers in the forward pass for distilBERT\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Additional layers for classification\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.3\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m, in \u001b[0;36mDistillBERTClass.add_query_layers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m q_lin \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mq_lin\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Freezing the original query layer weights, and adding LoRA layers\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mq_lin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     33\u001b[0m q_lin\u001b[38;5;241m.\u001b[39mbase_layer\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Add new LoRA layers (A and B matrices)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'base_layer'"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "VALID_BATCH_SIZE = 10\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "\n",
    "\n",
    "model = DistillBERTClass()\n",
    "model.to(device)\n",
    "\n",
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.35)\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUWKDy2QBL6k",
    "outputId": "0e2e9bed-41a3-4608-b03c-36896aaf047b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1.embeddings.word_embeddings.weight: requires_grad=False\n",
      "l1.embeddings.position_embeddings.weight: requires_grad=False\n",
      "l1.embeddings.LayerNorm.weight: requires_grad=False\n",
      "l1.embeddings.LayerNorm.bias: requires_grad=False\n",
      "l1.transformer.layer.0.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.0.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.0.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.0.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.0.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.0.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.0.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.0.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.0.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.0.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.0.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.0.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.0.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.0.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.0.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.0.output_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.1.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.1.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.1.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.1.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.1.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.1.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.1.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.1.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.1.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.1.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.1.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.1.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.1.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.1.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.1.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.1.output_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.2.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.2.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.2.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.2.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.2.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.2.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.2.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.2.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.2.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.2.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.2.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.2.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.2.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.2.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.2.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.2.output_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.3.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.3.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.3.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.3.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.3.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.3.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.3.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.3.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.3.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.3.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.3.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.3.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.3.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.3.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.3.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.3.output_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.4.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.4.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.4.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.4.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.4.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.4.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.4.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.4.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.4.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.4.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.4.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.4.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.4.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.4.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.4.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.4.output_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.5.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.5.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.5.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.5.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.5.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.5.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.5.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.5.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.5.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.5.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.5.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.5.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.5.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.5.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.5.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.5.output_layer_norm.bias: requires_grad=False\n",
      "pre_classifier.weight: requires_grad=True\n",
      "pre_classifier.bias: requires_grad=True\n",
      "fc1.weight: requires_grad=True\n",
      "fc1.bias: requires_grad=True\n",
      "classifier.weight: requires_grad=True\n",
      "classifier.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmGXgdNQCb7e"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "  model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgXdBTYUDLz6"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    #text = examples[\"body\"]\n",
    "    text = examples[\"body\"]\n",
    "    labels = examples[\"political_leaning\"]\n",
    "\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,#[preprocess(t) for t in text] ,\n",
    "        return_tensors = \"np\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 512\n",
    "        )\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = [label2id[label] for label in labels]\n",
    "    return tokenized_inputs\n",
    "\n",
    "#tokenized_dataset = df.map(tokenize_function, batched=True)\n",
    "#tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vf28kN8fBL6l"
   },
   "outputs": [],
   "source": [
    "# Define split ratio for validation\n",
    "train_test_split = df[\"train\"].train_test_split(test_size=0.1)  # 10% for validation\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": train_test_split[\"test\"],  # This is your validation set\n",
    "    \"test\": df[\"test\"],       # Keep the original test set\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4hEof0NBL6l"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define the mapping for political leaning categories to numeric values\n",
    "category_mapping = {\n",
    "    'LEFT': 0,\n",
    "    'CENTER': 1,\n",
    "    'RIGHT': 2,\n",
    "    'UNDEFINED': 3\n",
    "}\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    \"\"\" Preprocess the text to clean it for tokenization \"\"\"\n",
    "    def is_english_word(word):\n",
    "        \"\"\"Function to filter out non-English words.\"\"\"\n",
    "        return bool(re.match(r'^[a-zA-Z]+$', word))\n",
    "\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = contractions.fix(text)  # Expand contractions (e.g., \"don't\" -> \"do not\")\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = p.clean(text)  # Clean text using the clean-text library\n",
    "    return text\n",
    "\n",
    "class Triage(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.texts = dataset['body']  # Assuming 'text' column contains the raw text\n",
    "        self.labels = dataset['political_leaning']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get raw text and label for the current index\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        tokenizer.truncation_side = \"left\"\n",
    "        #tokenized_inputs = self.tokenizer(\n",
    "        tokenized_inputs = self.tokenizer.encode_plus(\n",
    "            preprocess(text),\n",
    "            None,\n",
    "            #return_tensors=\"pt\",\n",
    "            #padding=True,\n",
    "            #truncation=True,\n",
    "            #max_length=self.max_length\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "\n",
    "        #encoding = tokenize_function({\"text\": [text], \"labels\": [label]}, self.tokenizer, self.max_length)\n",
    "        input_ids = tokenized_inputs['input_ids']  # Remove the batch dimension\n",
    "        attention_mask = tokenized_inputs['attention_mask']  # Remove the batch dimension\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(category_mapping[self.labels[index]], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvrU6xHpBL6l"
   },
   "outputs": [],
   "source": [
    "train_dataset = Triage(datasets['train'], tokenizer, max_length=512)\n",
    "val_dataset = Triage(datasets['validation'], tokenizer, max_length=512)\n",
    "test_dataset = Triage(datasets['test'], tokenizer, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "On3GSFT7BL6l"
   },
   "outputs": [],
   "source": [
    "# Training DataLoader\n",
    "training_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# Validation DataLoader\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# Test DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iR8md4S6BL6m"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4hduUwTGnN5"
   },
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "def calculate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        #token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)#, token_type_ids)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calculate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "\n",
    "        if _%500==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples\n",
    "            print(f\"Training Loss per 500 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 500 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            #token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['labels'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calculate_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "\n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return epoch_loss, epoch_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kupJM6s-BL6m",
    "outputId": "4c9b160c-c896-49ea-ef79-df30aa362096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "1it [00:01,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.6161530017852783\n",
      "Training Accuracy per 500 steps: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [02:36,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.5428457041224557\n",
      "Training Accuracy per 500 steps: 32.375249500998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:10,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.5175689638673246\n",
      "Training Accuracy per 500 steps: 36.4035964035964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [07:50,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.4942098008561817\n",
      "Training Accuracy per 500 steps: 39.693537641572284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [10:28,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.4723683567061416\n",
      "Training Accuracy per 500 steps: 42.31384307846077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [13:09,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.4573322988328625\n",
      "Training Accuracy per 500 steps: 44.05037984806078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [15:56,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.4449079675858754\n",
      "Training Accuracy per 500 steps: 45.481506164611794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3501it [18:41,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.4355127770571803\n",
      "Training Accuracy per 500 steps: 46.441016852327905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [21:25,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.4271307935478745\n",
      "Training Accuracy per 500 steps: 47.32066983254187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4501it [24:11,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.4203222286592614\n",
      "Training Accuracy per 500 steps: 48.02710508775828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [26:59,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.4139577963428005\n",
      "Training Accuracy per 500 steps: 48.61427714457108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5501it [29:47,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.40909990642834\n",
      "Training Accuracy per 500 steps: 49.07289583712053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [32:36,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.403704816590665\n",
      "Training Accuracy per 500 steps: 49.61673054490918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6501it [35:25,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3986092389060834\n",
      "Training Accuracy per 500 steps: 50.10459929241655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [38:09,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3939991015193292\n",
      "Training Accuracy per 500 steps: 50.59277246107699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7501it [40:53,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3898318587994674\n",
      "Training Accuracy per 500 steps: 51.03586188508199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [43:37,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.387112283472746\n",
      "Training Accuracy per 500 steps: 51.31233595800525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8501it [46:19,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3838841073962607\n",
      "Training Accuracy per 500 steps: 51.66098106105164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [48:58,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3810516423933268\n",
      "Training Accuracy per 500 steps: 51.953116320408846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9501it [51:36,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.37823401204085\n",
      "Training Accuracy per 500 steps: 52.257657088727505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [54:15,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3758129998274224\n",
      "Training Accuracy per 500 steps: 52.504749525047494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10501it [57:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3730635547249694\n",
      "Training Accuracy per 500 steps: 52.775926102275974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11001it [59:44,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3705200910210642\n",
      "Training Accuracy per 500 steps: 53.00609035542224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11501it [1:02:31,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.368373766094485\n",
      "Training Accuracy per 500 steps: 53.21450308668811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11885it [1:04:38,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 0: 53.394872140086335\n",
      "Training Loss Epoch: 1.3666074358168199\n",
      "Training Accuracy Epoch: 53.394872140086335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 1.277788758277893\n",
      "Validation Accuracy per 100 steps: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1321it [06:59,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Epoch: 1.3072979011651154\n",
      "Validation Accuracy Epoch: 58.93979553199546\n",
      "Saved Best Model!\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2032194137573242\n",
      "Training Accuracy per 500 steps: 90.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [02:49,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.315928783483372\n",
      "Training Accuracy per 500 steps: 58.08383233532934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:40,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3130218085947332\n",
      "Training Accuracy per 500 steps: 58.63136863136863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [08:27,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3091357048553756\n",
      "Training Accuracy per 500 steps: 59.09393737508328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [11:12,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3062417942961713\n",
      "Training Accuracy per 500 steps: 59.44527736131934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [14:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3057544640615815\n",
      "Training Accuracy per 500 steps: 59.42423030787685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [16:48,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.305373816718979\n",
      "Training Accuracy per 500 steps: 59.480173275574806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3501it [19:37,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3059892589042814\n",
      "Training Accuracy per 500 steps: 59.41159668666096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [22:25,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3053471856014753\n",
      "Training Accuracy per 500 steps: 59.45013746563359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4501it [25:17,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.305383093977896\n",
      "Training Accuracy per 500 steps: 59.446789602310595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [28:35,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3041611463635998\n",
      "Training Accuracy per 500 steps: 59.542091581683664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5501it [31:42,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.304425466539556\n",
      "Training Accuracy per 500 steps: 59.538265769860026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [34:31,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.304336841623697\n",
      "Training Accuracy per 500 steps: 59.55507415430762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6501it [37:19,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3029152771598136\n",
      "Training Accuracy per 500 steps: 59.709275496077524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [40:09,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.302708188843138\n",
      "Training Accuracy per 500 steps: 59.73860877017569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7501it [43:03,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.302828518585624\n",
      "Training Accuracy per 500 steps: 59.72137048393547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [45:50,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3027352384888609\n",
      "Training Accuracy per 500 steps: 59.722534683164604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8501it [48:21,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.302699649197931\n",
      "Training Accuracy per 500 steps: 59.73297259145983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [50:55,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3025540443820802\n",
      "Training Accuracy per 500 steps: 59.735584935007225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9501it [53:26,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3017417118745331\n",
      "Training Accuracy per 500 steps: 59.82738659088517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [55:56,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.301117408157599\n",
      "Training Accuracy per 500 steps: 59.9020097990201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10501it [58:26,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3008907526439173\n",
      "Training Accuracy per 500 steps: 59.92000761832207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11001it [1:00:58,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.300824186995012\n",
      "Training Accuracy per 500 steps: 59.92364330515408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11501it [1:03:27,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3003459801550337\n",
      "Training Accuracy per 500 steps: 59.98521867663681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11885it [1:05:22,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 1: 60.049982750061005\n",
      "Training Loss Epoch: 1.2997019212337215\n",
      "Training Accuracy Epoch: 60.049982750061005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 1.2994295358657837\n",
      "Validation Accuracy per 100 steps: 70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1321it [07:11,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Epoch: 1.2805591528384999\n",
      "Validation Accuracy Epoch: 61.893222264293826\n",
      "Saved Best Model!\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.5001012086868286\n",
      "Training Accuracy per 500 steps: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [02:27,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2819078206540107\n",
      "Training Accuracy per 500 steps: 62.29540918163673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [04:53,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2862963100056073\n",
      "Training Accuracy per 500 steps: 61.75824175824176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [07:20,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2870408196515992\n",
      "Training Accuracy per 500 steps: 61.652231845436376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [09:47,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2873815163739142\n",
      "Training Accuracy per 500 steps: 61.58420789605197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [12:13,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.287135664533015\n",
      "Training Accuracy per 500 steps: 61.607357057177126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [14:39,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2862713568729387\n",
      "Training Accuracy per 500 steps: 61.67277574141953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3501it [17:07,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2857452420737532\n",
      "Training Accuracy per 500 steps: 61.65381319622965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [19:34,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2853905905994347\n",
      "Training Accuracy per 500 steps: 61.624593851537114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4501it [22:01,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.286024919723675\n",
      "Training Accuracy per 500 steps: 61.506331926238616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [24:29,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2857016411763957\n",
      "Training Accuracy per 500 steps: 61.54169166166766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5501it [26:55,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2848771393266598\n",
      "Training Accuracy per 500 steps: 61.6015269950918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [29:23,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.284538881557422\n",
      "Training Accuracy per 500 steps: 61.64305949008499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6501it [31:50,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2845531863499524\n",
      "Training Accuracy per 500 steps: 61.644362405783724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [34:18,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.285039492027025\n",
      "Training Accuracy per 500 steps: 61.568347378945866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7501it [36:46,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2848063242417973\n",
      "Training Accuracy per 500 steps: 61.59578722836955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [39:13,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.285065951689737\n",
      "Training Accuracy per 500 steps: 61.54230721159855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8501it [41:43,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2856692822733118\n",
      "Training Accuracy per 500 steps: 61.471591577461474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [44:09,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2857345832664295\n",
      "Training Accuracy per 500 steps: 61.45983779580047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9501it [46:37,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2856327539970342\n",
      "Training Accuracy per 500 steps: 61.460898852752344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [49:05,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2856004309587485\n",
      "Training Accuracy per 500 steps: 61.455854414558544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10501it [52:06,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2848961067510758\n",
      "Training Accuracy per 500 steps: 61.52366441291306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11001it [54:53,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.284922920241268\n",
      "Training Accuracy per 500 steps: 61.528951913462414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11501it [57:48,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2850420068589596\n",
      "Training Accuracy per 500 steps: 61.52769324406573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11885it [1:00:43,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 2: 61.5814407485632\n",
      "Training Loss Epoch: 1.2845688478706963\n",
      "Training Accuracy Epoch: 61.5814407485632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 1.2670718431472778\n",
      "Validation Accuracy per 100 steps: 70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1321it [09:30,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Epoch: 1.2745207325695682\n",
      "Validation Accuracy Epoch: 62.529344945096554\n",
      "Saved Best Model!\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.4189057350158691\n",
      "Training Accuracy per 500 steps: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [03:42,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2849637805344816\n",
      "Training Accuracy per 500 steps: 61.477045908183634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [06:09,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2870539551252846\n",
      "Training Accuracy per 500 steps: 61.408591408591406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [08:37,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.287765516153103\n",
      "Training Accuracy per 500 steps: 61.3057961359094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [12:30,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.283299863785282\n",
      "Training Accuracy per 500 steps: 61.75912043978011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [16:45,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2827683824484275\n",
      "Training Accuracy per 500 steps: 61.819272291083564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [19:14,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.282546733169308\n",
      "Training Accuracy per 500 steps: 61.86604465178274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3501it [21:43,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2811984007135864\n",
      "Training Accuracy per 500 steps: 62.01656669522993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [24:39,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2795644298668833\n",
      "Training Accuracy per 500 steps: 62.15696075981005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4501it [29:26,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.280028994137434\n",
      "Training Accuracy per 500 steps: 62.095089980004445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [33:54,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2796291888081963\n",
      "Training Accuracy per 500 steps: 62.13557288542292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5501it [38:19,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2793267525514458\n",
      "Training Accuracy per 500 steps: 62.16869660061807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [42:46,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2787563534324078\n",
      "Training Accuracy per 500 steps: 62.22796200633228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6501it [47:12,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2789686372914217\n",
      "Training Accuracy per 500 steps: 62.19812336563606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [51:40,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.280237092186495\n",
      "Training Accuracy per 500 steps: 62.01542636766176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7501it [56:08,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2801373528013418\n",
      "Training Accuracy per 500 steps: 62.01839754699373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [1:00:36,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2799388371159353\n",
      "Training Accuracy per 500 steps: 62.03599550056243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8501it [1:05:02,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.279969088197694\n",
      "Training Accuracy per 500 steps: 62.05034701799788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [1:09:32,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2800869122238612\n",
      "Training Accuracy per 500 steps: 62.041995333851794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9501it [1:14:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2799595127280368\n",
      "Training Accuracy per 500 steps: 62.04504788969582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [1:18:29,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.280078497067438\n",
      "Training Accuracy per 500 steps: 62.02379762023798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10501it [1:22:52,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2797841061729327\n",
      "Training Accuracy per 500 steps: 62.04456718407771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11001it [1:27:10,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2794494666237037\n",
      "Training Accuracy per 500 steps: 62.07072084355968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11501it [1:31:32,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2792476451393833\n",
      "Training Accuracy per 500 steps: 62.0841665942092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11885it [1:34:50,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 3: 62.071170723908416\n",
      "Training Loss Epoch: 1.2792797826948314\n",
      "Training Accuracy Epoch: 62.071170723908416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 1.2978764772415161\n",
      "Validation Accuracy per 100 steps: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1321it [10:54,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Epoch: 1.2680422133969864\n",
      "Validation Accuracy Epoch: 63.25634229458539\n",
      "Saved Best Model!\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.3714884519577026\n",
      "Training Accuracy per 500 steps: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [04:23,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.277561011190662\n",
      "Training Accuracy per 500 steps: 61.996007984031934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [08:47,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.270370101357078\n",
      "Training Accuracy per 500 steps: 62.82717282717283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [13:10,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.276824814053395\n",
      "Training Accuracy per 500 steps: 62.22518321119254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [17:31,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2788916100328531\n",
      "Training Accuracy per 500 steps: 61.95402298850575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [22:01,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2791457629260994\n",
      "Training Accuracy per 500 steps: 61.92323070771691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [26:30,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2783489058868602\n",
      "Training Accuracy per 500 steps: 61.97934021992669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3501it [31:02,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2782581099712043\n",
      "Training Accuracy per 500 steps: 62.050842616395315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [35:33,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2764977226105967\n",
      "Training Accuracy per 500 steps: 62.25693576605848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4501it [40:08,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.276311634395208\n",
      "Training Accuracy per 500 steps: 62.31948455898689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [44:38,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2758566223604872\n",
      "Training Accuracy per 500 steps: 62.39152169566087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5501it [49:08,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2766397576227206\n",
      "Training Accuracy per 500 steps: 62.314124704599166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [53:36,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2780841012494482\n",
      "Training Accuracy per 500 steps: 62.18463589401767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6501it [58:09,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.277408679198016\n",
      "Training Accuracy per 500 steps: 62.261190586063684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [1:13:06,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2772841739273126\n",
      "Training Accuracy per 500 steps: 62.291101271246966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7501it [1:24:51,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2773994006024822\n",
      "Training Accuracy per 500 steps: 62.27969604052793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [1:37:04,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.2771544552135432\n",
      "Training Accuracy per 500 steps: 62.3034620672416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8501it [1:48:50,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 500 steps: 1.277109654499999\n",
      "Training Accuracy per 500 steps: 62.315021762145626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8825it [1:56:51,  1.35s/it]"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    train_loss = train(epoch)\n",
    "    val_loss, val_accuracy = valid(model, val_loader)\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"./models/local_run_BERT_body/best_model.pt\")\n",
    "        print(\"Saved Best Model!\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhWv6-PHLtfq"
   },
   "outputs": [],
   "source": [
    "#!cp best_model.pt '/content/gdrive/MyDrive/ColabNotebooks/NLP Project/distilBERT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oETAzaqeBL6m",
    "outputId": "6582bbc4-d9a8-4565-f51e-6a6d9e116941"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-206-2172e1911a5f>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistillBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (fc1): Linear(in_features=768, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (classifier): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxTWh0CZBL6n",
    "outputId": "4a835e32-f0a2-4322-8d33-2ea54f5d937d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Test Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1796/1796 [04:12<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results\n",
      "------------------------------\n",
      "Accuracy: 0.5100\n",
      "Precision: 0.5181\n",
      "Recall: 0.5100\n",
      "F1-score: 0.4948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Test function\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Testing\"):\n",
    "            # Move batch to GPU/CPU\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    print(\"\\nTest Results\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# After training and validation, evaluate on the test set\n",
    "print(\"\\nEvaluating on Test Set\")\n",
    "test_accuracy, test_precision, test_recall, test_f1 = test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "PcInvd2hBL6n",
    "outputId": "9cb360f6-bc53-4553-d6a5-dac5225290a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Test Set\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-6967870fac08>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# After testing, plot the metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluating on Test Set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Metrics and their names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot function for metrics\n",
    "def plot_metrics(metrics, metric_names, title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bars = ax.bar(metric_names, metrics, color=['skyblue', 'orange', 'green', 'red'])\n",
    "\n",
    "    # Add value annotations on bars\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,\n",
    "                f\"{bar.get_height():.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_ylabel(\"Score\", fontsize=14)\n",
    "    ax.set_xlabel(\"Metrics\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# After testing, plot the metrics\n",
    "print(\"\\nEvaluating on Test Set\")\n",
    "test_accuracy, test_precision, test_recall, test_f1 = test_model(model, test_loader, device)\n",
    "\n",
    "# Metrics and their names\n",
    "metrics = [test_accuracy, test_precision, test_recall, test_f1]\n",
    "metric_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "\n",
    "# Plot the test results\n",
    "plot_metrics(metrics, metric_names, title=\"Test Metrics Overview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l84tjtX4BL6n"
   },
   "source": [
    "### Other form of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJWNUPl-BL6n"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "  predictions, labels = p\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return {\"accuracy\": accuracy.compute(predictions=predictions\n",
    "                                       , references=labels)}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmxsoKs4C1yh"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ORBVjXnGx19"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "lr = 1e-3\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"\"+model_checkpoint+\"lora-txt\",\n",
    "    learning_rate = lr,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    num_train_epochs = num_epochs,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    eval_dataset = tokenized_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmS4TS65IDDV"
   },
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JuqrjJEBL6o"
   },
   "source": [
    "### Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jn1iHMyJNtM"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Load model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dict = torch.load(\"trained_model_gral_imbd.pth\", map_location=device)\n",
    "\n",
    "text_list = ['''President-elect Trump announced on Tuesday night that he intends to appoint Linda McMahon, former CEO of World Wrestling Entertainment (WWE), to lead the Department of Education. His announcement, which was posted on Truth Social, came hours after two sources told Fox News that McMahon was likely to be picked. \"It is my great honor to announce that Linda McMahon, former Administrator of the Small Business Administration, will be the United States Secretary of Education,\" Trump's statement read.\n",
    "\"As Secretary of Education, Linda will fight tirelessly to expand Choice to every State in America, and empower parents to make the best Education decisions for their families,\" the press release added. \"Linda served for two years on the Connecticut Board of Education, where she was one of fifteen members overseeing all Public Education in the State, including its Technical High School system.\"''',\n",
    "             '''Donald Trump believes presidents have almost absolute power. In his second term, there will be few political or legal restraints to check him. The president-elects sweeping victory over Vice President Kamala Harris suddenly turned the theoretical notion that he will indulge his autocratic instincts into a genuine possibility.When Trump returns to the White House in January as one of the most powerful presidents in history, hell be able to take advantage of his own filleting of guardrails during his first presidency, which he continued through legal maneuverings out of office.''',\n",
    "             '''Nearly 100 Democrats, including Salud Carbajal, requested the Ethics Committee release its report on former Congressman Matt Gaetz's misconduct allegations. The letter, led by Rep. Sean Casten, emphasized that the Senate needs information for Gaetz's attorney general nomination. House Speaker Mike Johnson opposed releasing the report, stating Gaetz is now a \"private citizen\" and outside the panel's jurisdiction.'''\n",
    "             , ''' A South Dakota judge dismissed a lawsuit from the anti-abortion group Life Defense targeting an abortion rights measure that voters later rejected.\n",
    "Judge John Pekas dismissed the lawsuit at the request of Life Defense, which had challenged the ballot measure's petitions.\n",
    "Voters in nine states, including South Dakota, rejected abortion rights measures during the November election. '''\n",
    "             ]\n",
    "model.to('cuda')\n",
    "print('Trained model predictions')\n",
    "for text in text_list:\n",
    "  inputs = tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
    "\n",
    "  logits = model(inputs).logits\n",
    "  predictions = torch.max(logits,1).indices\n",
    "\n",
    "  #print(f'{text} - {id2label[predictions.tolist()[0]]}')\n",
    "  print(f'{id2label[predictions.tolist()[0]]}')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_lda_implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
