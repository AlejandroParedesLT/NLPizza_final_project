{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-24T17:19:10.546695Z",
     "iopub.status.busy": "2024-11-24T17:19:10.545960Z",
     "iopub.status.idle": "2024-11-24T17:19:10.556162Z",
     "shell.execute_reply": "2024-11-24T17:19:10.555396Z",
     "shell.execute_reply.started": "2024-11-24T17:19:10.546659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lr-model/trained_model_gral_imbd_LR_ONLY.pth\n",
      "/kaggle/input/2017-1/2017_1.csv\n",
      "/kaggle/input/2017-2/2017_2.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:19:10.557833Z",
     "iopub.status.busy": "2024-11-24T17:19:10.557550Z",
     "iopub.status.idle": "2024-11-24T17:19:10.565587Z",
     "shell.execute_reply": "2024-11-24T17:19:10.564688Z",
     "shell.execute_reply.started": "2024-11-24T17:19:10.557807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:19:10.566791Z",
     "iopub.status.busy": "2024-11-24T17:19:10.566525Z",
     "iopub.status.idle": "2024-11-24T17:19:26.944250Z",
     "shell.execute_reply": "2024-11-24T17:19:26.943092Z",
     "shell.execute_reply.started": "2024-11-24T17:19:10.566766Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers \n",
    "!pip install datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:19:26.946730Z",
     "iopub.status.busy": "2024-11-24T17:19:26.946394Z",
     "iopub.status.idle": "2024-11-24T17:19:43.281451Z",
     "shell.execute_reply": "2024-11-24T17:19:43.280223Z",
     "shell.execute_reply.started": "2024-11-24T17:19:26.946701Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:19:43.284006Z",
     "iopub.status.busy": "2024-11-24T17:19:43.283121Z",
     "iopub.status.idle": "2024-11-24T17:19:51.065673Z",
     "shell.execute_reply": "2024-11-24T17:19:51.064954Z",
     "shell.execute_reply.started": "2024-11-24T17:19:43.283951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:19:51.067203Z",
     "iopub.status.busy": "2024-11-24T17:19:51.066678Z",
     "iopub.status.idle": "2024-11-24T17:19:51.071235Z",
     "shell.execute_reply": "2024-11-24T17:19:51.070276Z",
     "shell.execute_reply.started": "2024-11-24T17:19:51.067175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:19:51.072541Z",
     "iopub.status.busy": "2024-11-24T17:19:51.072260Z",
     "iopub.status.idle": "2024-11-24T17:19:53.069761Z",
     "shell.execute_reply": "2024-11-24T17:19:53.069110Z",
     "shell.execute_reply.started": "2024-11-24T17:19:51.072515Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bdf6299127440e90e3ba4accef91fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79c8fdb068a45939474cf42b97e2c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "# Define label maps\n",
    "id2label = {0: \"LEFT\", 1: \"RIGHT\"}\n",
    "label2id = {\"LEFT\": 0, \"RIGHT\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:19:53.072255Z",
     "iopub.status.busy": "2024-11-24T17:19:53.071989Z",
     "iopub.status.idle": "2024-11-24T17:20:23.334736Z",
     "shell.execute_reply": "2024-11-24T17:20:23.334045Z",
     "shell.execute_reply.started": "2024-11-24T17:19:53.072229Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f42338119404bf7b6024cca350d3c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a44cd4964f44391b5abeaa362361f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df1 = load_dataset(\n",
    "    \"csv\", data_files=\"/kaggle/input/2017-1/2017_1.csv\", encoding=\"utf-8\"\n",
    ")[\"train\"]\n",
    "df2 = load_dataset(\n",
    "    \"csv\", data_files=\"/kaggle/input/2017-2/2017_2.csv\", encoding=\"utf-8\"\n",
    ")[\"train\"]\n",
    "df = concatenate_datasets([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:20:23.335882Z",
     "iopub.status.busy": "2024-11-24T17:20:23.335654Z",
     "iopub.status.idle": "2024-11-24T17:20:27.834228Z",
     "shell.execute_reply": "2024-11-24T17:20:27.833375Z",
     "shell.execute_reply.started": "2024-11-24T17:20:23.335858Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b276b6f343a444d0a04e0c751070b823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/326240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_df = df.filter(lambda x: x[\"political_leaning\"] in [\"LEFT\", \"RIGHT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:20:27.835561Z",
     "iopub.status.busy": "2024-11-24T17:20:27.835257Z",
     "iopub.status.idle": "2024-11-24T17:20:27.841572Z",
     "shell.execute_reply": "2024-11-24T17:20:27.840719Z",
     "shell.execute_reply.started": "2024-11-24T17:20:27.835533Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "    num_rows: 154401\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:20:27.842673Z",
     "iopub.status.busy": "2024-11-24T17:20:27.842438Z",
     "iopub.status.idle": "2024-11-24T17:20:27.900046Z",
     "shell.execute_reply": "2024-11-24T17:20:27.899354Z",
     "shell.execute_reply.started": "2024-11-24T17:20:27.842649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:20:27.901319Z",
     "iopub.status.busy": "2024-11-24T17:20:27.901061Z",
     "iopub.status.idle": "2024-11-24T17:20:27.976563Z",
     "shell.execute_reply": "2024-11-24T17:20:27.975687Z",
     "shell.execute_reply.started": "2024-11-24T17:20:27.901292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df[\"test\"][\"political_leaning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:20:27.978021Z",
     "iopub.status.busy": "2024-11-24T17:20:27.977757Z",
     "iopub.status.idle": "2024-11-24T17:20:29.255527Z",
     "shell.execute_reply": "2024-11-24T17:20:29.254595Z",
     "shell.execute_reply.started": "2024-11-24T17:20:27.977994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf5fcd473984577b7bf64b268263ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6254588b931e487d8fa18d80ac676a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1265a856e6b444f8a514c03e9dc7233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:20:29.256808Z",
     "iopub.status.busy": "2024-11-24T17:20:29.256538Z",
     "iopub.status.idle": "2024-11-24T17:20:29.261421Z",
     "shell.execute_reply": "2024-11-24T17:20:29.260545Z",
     "shell.execute_reply.started": "2024-11-24T17:20:29.256781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    text = examples[\"headline\"]\n",
    "    labels = examples[\"political_leaning\"]\n",
    "\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text, return_tensors=\"np\", padding=True, truncation=True, max_length=512\n",
    "    )\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = [label2id[label] for label in labels]\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:20:29.262681Z",
     "iopub.status.busy": "2024-11-24T17:20:29.262375Z",
     "iopub.status.idle": "2024-11-24T17:20:29.270984Z",
     "shell.execute_reply": "2024-11-24T17:20:29.270316Z",
     "shell.execute_reply.started": "2024-11-24T17:20:29.262656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:20:29.272268Z",
     "iopub.status.busy": "2024-11-24T17:20:29.271957Z",
     "iopub.status.idle": "2024-11-24T17:20:49.026918Z",
     "shell.execute_reply": "2024-11-24T17:20:49.026101Z",
     "shell.execute_reply.started": "2024-11-24T17:20:29.272242Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50066e87d7914082963bf2203bc13935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3538f45b2914975b2809d3f39594f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 138960\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15441\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = filtered_df.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:20:49.028234Z",
     "iopub.status.busy": "2024-11-24T17:20:49.027979Z",
     "iopub.status.idle": "2024-11-24T17:20:49.032090Z",
     "shell.execute_reply": "2024-11-24T17:20:49.031318Z",
     "shell.execute_reply.started": "2024-11-24T17:20:49.028208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:20:49.033369Z",
     "iopub.status.busy": "2024-11-24T17:20:49.033096Z",
     "iopub.status.idle": "2024-11-24T17:20:51.838190Z",
     "shell.execute_reply": "2024-11-24T17:20:51.835917Z",
     "shell.execute_reply.started": "2024-11-24T17:20:49.033324Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '‘' (U+2018) (4119629576.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=‘weighted’)\u001b[0m\n\u001b[0m                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '‘' (U+2018)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def compute_metrics(pred):\n",
    "labels = pred.label_ids\n",
    "preds = pred.predictions.argmax(-1)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=‘weighted’)\n",
    "acc = accuracy_score(labels, preds)\n",
    "return {\n",
    "‘accuracy’: acc,\n",
    "‘f1’: f1,\n",
    "‘precision’: precision,\n",
    "‘recall’: recall\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:50:31.873907Z",
     "iopub.status.busy": "2024-11-24T17:50:31.873232Z",
     "iopub.status.idle": "2024-11-24T17:50:34.626352Z",
     "shell.execute_reply": "2024-11-24T17:50:34.625536Z",
     "shell.execute_reply.started": "2024-11-24T17:50:31.873873Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de30d765e9f4f828e225943ed741707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a496dffc3904e30b0ff6471a16b0e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fc002a4758474fb41c2f8e84c9678a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686646f101a04456bf7eb0a3f334b8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels),\n",
    "        \"precision\": precision.compute(predictions=predictions, references=labels),\n",
    "        \"f1_score\": f1_metric.compute(predictions=predictions, references=labels),\n",
    "        \"recall\": recall.compute(predictions=predictions, references=labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:51:02.003051Z",
     "iopub.status.busy": "2024-11-24T17:51:02.002710Z",
     "iopub.status.idle": "2024-11-24T17:51:02.131928Z",
     "shell.execute_reply": "2024-11-24T17:51:02.131048Z",
     "shell.execute_reply.started": "2024-11-24T17:51:02.003020Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model\n",
      "1\n",
      "It was good. - RIGHT\n",
      "1\n",
      "Not a fan, don't recommended - RIGHT\n",
      "1\n",
      "Better than the first one. - RIGHT\n",
      "1\n",
      "Women have the right to choose and abortion should be allowed. - RIGHT\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"It was good.\",\n",
    "    \"Not a fan, don't recommended\",\n",
    "    \"Better than the first one.\",\n",
    "    \"Women have the right to choose and abortion should be allowed.\",\n",
    "]\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Untrained model\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\n",
    "        device\n",
    "    )  # Move inputs to the correct device\n",
    "    logits = model(**inputs).logits  # Forward pass\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    print(predictions.item())\n",
    "    print(f\"{text} - {id2label[predictions.item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:51:05.892051Z",
     "iopub.status.busy": "2024-11-24T17:51:05.891367Z",
     "iopub.status.idle": "2024-11-24T17:51:05.896267Z",
     "shell.execute_reply": "2024-11-24T17:51:05.895259Z",
     "shell.execute_reply.started": "2024-11-24T17:51:05.891996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\", r=4, lora_alpha=32, lora_dropout=0.01, target_modules=[\"q_lin\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:51:08.270898Z",
     "iopub.status.busy": "2024-11-24T17:51:08.270558Z",
     "iopub.status.idle": "2024-11-24T17:51:08.294524Z",
     "shell.execute_reply": "2024-11-24T17:51:08.293596Z",
     "shell.execute_reply.started": "2024-11-24T17:51:08.270868Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9307\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:51:12.555686Z",
     "iopub.status.busy": "2024-11-24T17:51:12.555345Z",
     "iopub.status.idle": "2024-11-24T17:51:12.636964Z",
     "shell.execute_reply": "2024-11-24T17:51:12.636112Z",
     "shell.execute_reply.started": "2024-11-24T17:51:12.555658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 10\n",
    "num_epochs = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"\" + model_checkpoint + \"lora-txt\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:51:19.685055Z",
     "iopub.status.busy": "2024-11-24T17:51:19.684249Z",
     "iopub.status.idle": "2024-11-24T17:51:20.956307Z",
     "shell.execute_reply": "2024-11-24T17:51:20.955414Z",
     "shell.execute_reply.started": "2024-11-24T17:51:19.685019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T17:51:26.741751Z",
     "iopub.status.busy": "2024-11-24T17:51:26.741444Z",
     "iopub.status.idle": "2024-11-24T18:18:19.629637Z",
     "shell.execute_reply": "2024-11-24T18:18:19.628905Z",
     "shell.execute_reply.started": "2024-11-24T17:51:26.741724Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0e24ea30cd4c3abe2d4cb988bd5a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111330088889291, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241124_175153-scn7f91x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jchen63-duke/huggingface/runs/scn7f91x' target=\"_blank\">distilbert-base-uncasedlora-txt</a></strong> to <a href='https://wandb.ai/jchen63-duke/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jchen63-duke/huggingface' target=\"_blank\">https://wandb.ai/jchen63-duke/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jchen63-duke/huggingface/runs/scn7f91x' target=\"_blank\">https://wandb.ai/jchen63-duke/huggingface/runs/scn7f91x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69480' max='69480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69480/69480 26:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>0.482327</td>\n",
       "      <td>{'accuracy': 0.7691211709086199}</td>\n",
       "      <td>{'precision': 0.6923326835607537}</td>\n",
       "      <td>{'f1': 0.5445253609301137}</td>\n",
       "      <td>{'recall': 0.4487260475889661}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>0.456236</td>\n",
       "      <td>{'accuracy': 0.7790298555793018}</td>\n",
       "      <td>{'precision': 0.6878336611407698}</td>\n",
       "      <td>{'f1': 0.5893115069812228}</td>\n",
       "      <td>{'recall': 0.5154769425142135}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.449200</td>\n",
       "      <td>0.453251</td>\n",
       "      <td>{'accuracy': 0.7847289683310666}</td>\n",
       "      <td>{'precision': 0.6886417791898332}</td>\n",
       "      <td>{'f1': 0.6101337086558761}</td>\n",
       "      <td>{'recall': 0.5476942514213519}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.429300</td>\n",
       "      <td>0.435054</td>\n",
       "      <td>{'accuracy': 0.7949614662262807}</td>\n",
       "      <td>{'precision': 0.7165526675786593}</td>\n",
       "      <td>{'f1': 0.6232746311280342}</td>\n",
       "      <td>{'recall': 0.5514845230574857}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.425993</td>\n",
       "      <td>{'accuracy': 0.8039634738682728}</td>\n",
       "      <td>{'precision': 0.7259842519685039}</td>\n",
       "      <td>{'f1': 0.6463371889239398}</td>\n",
       "      <td>{'recall': 0.5824384080859129}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.7691211709086199}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6923326835607537}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.5445253609301137}\" of type <class 'dict'> for key \"eval/f1_score\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.4487260475889661}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.7790298555793018}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6878336611407698}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.5893115069812228}\" of type <class 'dict'> for key \"eval/f1_score\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.5154769425142135}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.7847289683310666}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6886417791898332}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.6101337086558761}\" of type <class 'dict'> for key \"eval/f1_score\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.5476942514213519}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.7949614662262807}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7165526675786593}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.6232746311280342}\" of type <class 'dict'> for key \"eval/f1_score\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.5514845230574857}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.8039634738682728}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7259842519685039}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.6463371889239398}\" of type <class 'dict'> for key \"eval/f1_score\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.5824384080859129}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=69480, training_loss=0.4560226761066303, metrics={'train_runtime': 1612.2486, 'train_samples_per_second': 430.951, 'train_steps_per_second': 43.095, 'total_flos': 6863103307897440.0, 'train_loss': 0.4560226761066303, 'epoch': 5.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T18:22:05.202885Z",
     "iopub.status.busy": "2024-11-24T18:22:05.202542Z",
     "iopub.status.idle": "2024-11-24T18:22:20.158715Z",
     "shell.execute_reply": "2024-11-24T18:22:20.157937Z",
     "shell.execute_reply.started": "2024-11-24T18:22:05.202854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.8039634738682728}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7259842519685039}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.6463371889239398}\" of type <class 'dict'> for key \"eval/f1_score\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.5824384080859129}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4259926378726959,\n",
       " 'eval_accuracy': {'accuracy': 0.8039634738682728},\n",
       " 'eval_precision': {'precision': 0.7259842519685039},\n",
       " 'eval_f1_score': {'f1': 0.6463371889239398},\n",
       " 'eval_recall': {'recall': 0.5824384080859129},\n",
       " 'eval_runtime': 14.939,\n",
       " 'eval_samples_per_second': 1033.606,\n",
       " 'eval_steps_per_second': 103.421,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T18:23:05.022125Z",
     "iopub.status.busy": "2024-11-24T18:23:05.021302Z",
     "iopub.status.idle": "2024-11-24T18:23:22.485775Z",
     "shell.execute_reply": "2024-11-24T18:23:22.485018Z",
     "shell.execute_reply.started": "2024-11-24T18:23:05.022087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T18:24:07.140607Z",
     "iopub.status.busy": "2024-11-24T18:24:07.140261Z",
     "iopub.status.idle": "2024-11-24T18:24:07.147751Z",
     "shell.execute_reply": "2024-11-24T18:24:07.146915Z",
     "shell.execute_reply.started": "2024-11-24T18:24:07.140576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T18:23:35.445379Z",
     "iopub.status.busy": "2024-11-24T18:23:35.444986Z",
     "iopub.status.idle": "2024-11-24T18:23:35.577098Z",
     "shell.execute_reply": "2024-11-24T18:23:35.576086Z",
     "shell.execute_reply.started": "2024-11-24T18:23:35.445342Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Trained model predictions\n",
      "It was good. - LEFT\n",
      "Not a fan, don't recommended - LEFT\n",
      "Better than the first one. - LEFT\n",
      "Women have the right to choose and abortion should be allowed. - LEFT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "print(\"Trained model predictions\")\n",
    "\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    print(f\"{text} - {id2label[predictions.item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T18:51:15.719541Z",
     "iopub.status.busy": "2024-11-24T18:51:15.718821Z",
     "iopub.status.idle": "2024-11-24T18:51:17.078455Z",
     "shell.execute_reply": "2024-11-24T18:51:17.077510Z",
     "shell.execute_reply.started": "2024-11-24T18:51:15.719506Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n"
     ]
    }
   ],
   "source": [
    "output_model_file = \"pytorch_distilbert_imbd.bin\"\n",
    "output_vocab_file = \"/kaggle/working/vocab_distilbert_imbd.bin\"\n",
    "\n",
    "# Save model\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "\n",
    "# Save tokenizer vocabulary in the current directory\n",
    "tokenizer.save_vocabulary(\".\")  # Current directory\n",
    "\n",
    "# Save model state dictionary\n",
    "torch.save(model.state_dict(), \"/kaggle/working/trained_model_gral_imbd_HEAD.pth\")\n",
    "\n",
    "print(\"All files saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
