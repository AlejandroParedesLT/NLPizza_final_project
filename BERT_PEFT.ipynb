{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alejandro Paredes, Parameter tuning of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwAV24NJ-ZSj",
    "outputId": "f707f622-fcd8-4645-fc3f-6beaef291c8d"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets peft evaluate #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C2gg1Syx-s44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KfUh_vrS_hbR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "#Define label maps\n",
    "id2label = {0:\"UNDEFINED\" ,1:\"LEFT\",2:\"RIGHT\",3:\"CENTER\"}\n",
    "label2id = {\"UNDEFINED\": 0, \"LEFT\": 1, \"RIGHT\": 2, \"CENTER\": 3}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=4, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yrj-hSULAi_a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "    num_rows: 50269\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df = load_dataset(\"csv\", data_files=\"./data/2019_2.csv\", split=\"train\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 50269/50269 [00:01<00:00, 27238.80 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "        num_rows: 45242\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "        num_rows: 5027\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and combine the datasets\n",
    "dataset = load_dataset(\"csv\", data_files=\"./data/2019_2.csv\")#data_files)\n",
    "\n",
    "# Filter and split the dataset\n",
    "df  = dataset['train'].filter(\n",
    "    lambda example: example['headline'] is not None and example['headline'].strip() != ''\n",
    ").train_test_split(test_size=0.1)\n",
    "\n",
    "# Display the resulting dataset\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  For many, it's clear why El Paso, the \"ground zero\" of the border debate, was the shooting target \n",
      "\n",
      "Tokenized Text:  ['for', 'many', ',', 'it', \"'\", 's', 'clear', 'why', 'el', 'paso', ',', 'the', '\"', 'ground', 'zero', '\"', 'of', 'the', 'border', 'debate', ',', 'was', 'the', 'shooting', 'target'] \n",
      "\n",
      "Token IDs:  [2005, 2116, 1010, 2009, 1005, 1055, 3154, 2339, 3449, 17161, 1010, 1996, 1000, 2598, 5717, 1000, 1997, 1996, 3675, 5981, 1010, 2001, 1996, 5008, 4539]\n",
      "Original Text:  Politics of automation: Factory workers and robots \n",
      "\n",
      "Tokenized Text:  ['politics', 'of', 'automation', ':', 'factory', 'workers', 'and', 'robots'] \n",
      "\n",
      "Token IDs:  [4331, 1997, 19309, 1024, 4713, 3667, 1998, 13507]\n",
      "Original Text:  'Straight Pride Parade' in Boston draws counterprotesters and heavy police presence \n",
      "\n",
      "Tokenized Text:  [\"'\", 'straight', 'pride', 'parade', \"'\", 'in', 'boston', 'draws', 'counter', '##pro', '##test', '##ers', 'and', 'heavy', 'police', 'presence'] \n",
      "\n",
      "Token IDs:  [1005, 3442, 6620, 7700, 1005, 1999, 3731, 9891, 4675, 21572, 22199, 2545, 1998, 3082, 2610, 3739]\n",
      "Original Text:  Dalradian: Tyrone gold mine company drop cyanide plan \n",
      "\n",
      "Tokenized Text:  ['dal', '##rad', '##ian', ':', 'tyrone', 'gold', 'mine', 'company', 'drop', 'cy', '##ani', '##de', 'plan'] \n",
      "\n",
      "Token IDs:  [17488, 12173, 2937, 1024, 18770, 2751, 3067, 2194, 4530, 22330, 7088, 3207, 2933]\n",
      "Original Text:  Epstein's death officially ruled a suicide, conspiracy theories dashed \n",
      "\n",
      "Tokenized Text:  ['epstein', \"'\", 's', 'death', 'officially', 'ruled', 'a', 'suicide', ',', 'conspiracy', 'theories', 'dashed'] \n",
      "\n",
      "Token IDs:  [26646, 1005, 1055, 2331, 3985, 5451, 1037, 5920, 1010, 9714, 8106, 18198]\n",
      "Original Text:  When a gunman stormed a crowded Walmart in El Paso on Saturday, killing at least 20 people and injuring more than two dozen others, the Texas border city was hit with an unprecedented level of bloodshed and grief. Along with another mass shooting in Dayton, Ohio some 13 hours later, the massacre in El Paso reignited the highly contentious national debate around proposals to regulate guns.\n",
      "El Paso is a border community unaccustomed to such large-scale acts of violence.\n",
      "And for many residents of El Paso — an epicenter of another of the nation's most divisive issues, immigration — the gruesome attack not only underscored the need to restrict access to high-caliber weapons like the one used by the alleged assailant, it also represented a clear and direct assault on the city's diversity and its standing as a welcoming community for migrants.\n",
      "\"We really don't have to guess. We know. We were targeted because the terrorist wanted to attack a mostly Latino and immigrant community,\" Democratic Rep. Veronica Escobar, who represents El Paso, told CBS News in an interview.\n",
      "El Paso, Spanish for \"the pass,\" stands at the intersection of two U.S. states, Texas and New Mexico, and shares an international border with the Mexican city of Ciudad Juárez. Camilo Montoya-Galvez\n",
      "Nestled in the Chihuahuan Desert, El Paso, Spanish for \"the pass,\" sits at the intersection of two U.S. states, Texas and New Mexico, and shares an international border with Ciudad Juárez, the largest city in the Mexican state of Chihuahua. In this predominantly Latino community, home to a large group of binational workers and bilingual residents, many business signs are in both English and Spanish and family-owned Mexican eateries stand alongside hipster coffee shops.\n",
      "Escobar and other community members believe their city's symbolism as a beacon of multiculturalism and strong binational ties attracted the suspected shooter, a 21-year-old white man who is in government custody.\n",
      "Many of the small businesses in the predominately Latino border city of El Paso have signage in both English and Spanish. Camilo Montoya-Galvez\n",
      "The deadly rampage is being treated as an act of domestic terrorism by the Justice Department and a potential hate crime by federal investigators, who are probing a racist, anti-immigrant document purportedly authored by the suspect. The alleged manifesto decries the growing political power of Texas' large Latino community and denounces progressive positions on immigration.\n",
      "Like other high-profile Democrats, Escobar drew a direct link between President Trump's hard-line and often inflammatory rhetoric on immigration and the apparent motives of the gunman. In fiery rally speeches and official proclamations touting his hard-line immigration agenda, the president has employed words like \"invasion\" to describe the movement of migrants.\n",
      "\"We have been talked about a lot by the president. We have been ground zero for the Trump administration's anti-immigrant agenda,\" the Texas Democrat said. \"That means we've been in the news a lot.\"\n",
      "Escobar, one of two of the first Latinas to represent Texas in Congress, said the El Paso community's response to a months-long surge of migrants — particularly families with small children — heading toward the southern border was also most likely targeted by the gunman. She said she herself has been targeted because of her advocacy in this field.\n",
      "\"I've been the subject of death threats because I have essentially called for our better angels during these challenging times,\" she said. \"I've been attacked for asking people to treat migrants with compassion. There's something very sick happening to our country.\"\n",
      "Located in one of the most highly transited parts of the U.S.-Mexico border, El Paso has fed and sheltered tens of thousands of migrant families from all corners of the globe, particularly Central America, this year alone. Most of them have been assisted by the Annunciation House network of shelters, which in the late winter and early spring managed to house — partly by spending more than $1 million in hotel rooms — thousands of migrant families released by Immigration and Customs Enforcement (ICE).\n",
      "The Annunciation House director, Ruben Garcia, who was born and raised in the city, called the massacre an \"obscenity\" and stressed that the alleged shooter, a resident of the Dallas suburb of Allen, is not a member of the El Paso community.\n",
      "\"This is someone who came out of El Paso to do harm to our city, to our people, to our way of life,\" Garcia told CBS News in an interview. \"And I think first, that in itself is an acknowledgement of who the people of the city of El Paso are.\"\n",
      "Annunciation House director Ruben Garcia, left, stands alongside Democratic Sens. Mazie Hirono of Hawaii and Tim Kaine of Virginia to talk to a migrant family at the \"Casa del Refugiado\" shelter in El Paso. Camilo Montoya-Galvez\n",
      "Like Escobar, the shelter director described El Paso as \"ground zero\" for the recent large influx of asylum-seeking families and the administration's efforts to stem their migration. He noted that U.S. officials implemented a pilot program in the area in late 2017 that served as a precursor to the administration's controversial \"zero tolerance\" policy, which led to the forcible separation of nearly 3,000 migrant families.\n",
      "Dylan Corbett, executive director of the El Paso-based non-profit Hope Border Institute said the Texas border city's response to the unprecedented flow of migrants that began late last year serves as a stark alternative to the administration's deterrence efforts.\n",
      "\"The way that is being presented to us by the people in power in the White House is that we have respond with fear. And we have to respond with demonization. And that we have to treat these people like criminals. And that we're somehow going to militarize to solve these problems. When on the ground, the reality here has been quite different,\" Corbett told CBS News in an interview.\n",
      "Dozens of murals, including the one above, adorn El Paso, paying tribute to the city's diversity and ties to neighboring Mexico. Camilo Montoya-Galvez\n",
      "If the gunman targeted El Paso partly because of the community's advocacy and work with migrants, Corbett said, the city would not surrender the values he believes defines it.\n",
      "\"If we've been attacked because of that, because of our compassion, because of generosity, then I think we need to double down on that,\" he said. \"And we think we can show the rest for the country that these values are worth fighting for.\" \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1374 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text:  ['when', 'a', 'gun', '##man', 'stormed', 'a', 'crowded', 'wal', '##mart', 'in', 'el', 'paso', 'on', 'saturday', ',', 'killing', 'at', 'least', '20', 'people', 'and', 'injuring', 'more', 'than', 'two', 'dozen', 'others', ',', 'the', 'texas', 'border', 'city', 'was', 'hit', 'with', 'an', 'unprecedented', 'level', 'of', 'blood', '##shed', 'and', 'grief', '.', 'along', 'with', 'another', 'mass', 'shooting', 'in', 'dayton', ',', 'ohio', 'some', '13', 'hours', 'later', ',', 'the', 'massacre', 'in', 'el', 'paso', 'reign', '##ited', 'the', 'highly', 'contentious', 'national', 'debate', 'around', 'proposals', 'to', 'regulate', 'guns', '.', 'el', 'paso', 'is', 'a', 'border', 'community', 'una', '##ccus', '##tom', '##ed', 'to', 'such', 'large', '-', 'scale', 'acts', 'of', 'violence', '.', 'and', 'for', 'many', 'residents', 'of', 'el', 'paso', '—', 'an', 'epic', '##enter', 'of', 'another', 'of', 'the', 'nation', \"'\", 's', 'most', 'di', '##vis', '##ive', 'issues', ',', 'immigration', '—', 'the', 'gr', '##ues', '##ome', 'attack', 'not', 'only', 'under', '##sco', '##red', 'the', 'need', 'to', 'restrict', 'access', 'to', 'high', '-', 'caliber', 'weapons', 'like', 'the', 'one', 'used', 'by', 'the', 'alleged', 'ass', '##ail', '##ant', ',', 'it', 'also', 'represented', 'a', 'clear', 'and', 'direct', 'assault', 'on', 'the', 'city', \"'\", 's', 'diversity', 'and', 'its', 'standing', 'as', 'a', 'welcoming', 'community', 'for', 'migrants', '.', '\"', 'we', 'really', 'don', \"'\", 't', 'have', 'to', 'guess', '.', 'we', 'know', '.', 'we', 'were', 'targeted', 'because', 'the', 'terrorist', 'wanted', 'to', 'attack', 'a', 'mostly', 'latino', 'and', 'immigrant', 'community', ',', '\"', 'democratic', 'rep', '.', 'veronica', 'es', '##co', '##bar', ',', 'who', 'represents', 'el', 'paso', ',', 'told', 'cbs', 'news', 'in', 'an', 'interview', '.', 'el', 'paso', ',', 'spanish', 'for', '\"', 'the', 'pass', ',', '\"', 'stands', 'at', 'the', 'intersection', 'of', 'two', 'u', '.', 's', '.', 'states', ',', 'texas', 'and', 'new', 'mexico', ',', 'and', 'shares', 'an', 'international', 'border', 'with', 'the', 'mexican', 'city', 'of', 'ciudad', 'juarez', '.', 'cam', '##ilo', 'mont', '##oya', '-', 'gal', '##vez', 'nestled', 'in', 'the', 'chihuahua', '##n', 'desert', ',', 'el', 'paso', ',', 'spanish', 'for', '\"', 'the', 'pass', ',', '\"', 'sits', 'at', 'the', 'intersection', 'of', 'two', 'u', '.', 's', '.', 'states', ',', 'texas', 'and', 'new', 'mexico', ',', 'and', 'shares', 'an', 'international', 'border', 'with', 'ciudad', 'juarez', ',', 'the', 'largest', 'city', 'in', 'the', 'mexican', 'state', 'of', 'chihuahua', '.', 'in', 'this', 'predominantly', 'latino', 'community', ',', 'home', 'to', 'a', 'large', 'group', 'of', 'bin', '##ation', '##al', 'workers', 'and', 'bilingual', 'residents', ',', 'many', 'business', 'signs', 'are', 'in', 'both', 'english', 'and', 'spanish', 'and', 'family', '-', 'owned', 'mexican', 'eater', '##ies', 'stand', 'alongside', 'hips', '##ter', 'coffee', 'shops', '.', 'es', '##co', '##bar', 'and', 'other', 'community', 'members', 'believe', 'their', 'city', \"'\", 's', 'symbolism', 'as', 'a', 'beacon', 'of', 'multicultural', '##ism', 'and', 'strong', 'bin', '##ation', '##al', 'ties', 'attracted', 'the', 'suspected', 'shooter', ',', 'a', '21', '-', 'year', '-', 'old', 'white', 'man', 'who', 'is', 'in', 'government', 'custody', '.', 'many', 'of', 'the', 'small', 'businesses', 'in', 'the', 'pre', '##dom', '##inate', '##ly', 'latino', 'border', 'city', 'of', 'el', 'paso', 'have', 'signage', 'in', 'both', 'english', 'and', 'spanish', '.', 'cam', '##ilo', 'mont', '##oya', '-', 'gal', '##vez', 'the', 'deadly', 'rampage', 'is', 'being', 'treated', 'as', 'an', 'act', 'of', 'domestic', 'terrorism', 'by', 'the', 'justice', 'department', 'and', 'a', 'potential', 'hate', 'crime', 'by', 'federal', 'investigators', ',', 'who', 'are', 'probing', 'a', 'racist', ',', 'anti', '-', 'immigrant', 'document', 'purported', '##ly', 'authored', 'by', 'the', 'suspect', '.', 'the', 'alleged', 'manifesto', 'dec', '##ries', 'the', 'growing', 'political', 'power', 'of', 'texas', \"'\", 'large', 'latino', 'community', 'and', 'den', '##oun', '##ces', 'progressive', 'positions', 'on', 'immigration', '.', 'like', 'other', 'high', '-', 'profile', 'democrats', ',', 'es', '##co', '##bar', 'drew', 'a', 'direct', 'link', 'between', 'president', 'trump', \"'\", 's', 'hard', '-', 'line', 'and', 'often', 'inflammatory', 'rhetoric', 'on', 'immigration', 'and', 'the', 'apparent', 'motives', 'of', 'the', 'gun', '##man', '.', 'in', 'fiery', 'rally', 'speeches', 'and', 'official', 'proclamation', '##s', 'to', '##uting', 'his', 'hard', '-', 'line', 'immigration', 'agenda', ',', 'the', 'president', 'has', 'employed', 'words', 'like', '\"', 'invasion', '\"', 'to', 'describe', 'the', 'movement', 'of', 'migrants', '.', '\"', 'we', 'have', 'been', 'talked', 'about', 'a', 'lot', 'by', 'the', 'president', '.', 'we', 'have', 'been', 'ground', 'zero', 'for', 'the', 'trump', 'administration', \"'\", 's', 'anti', '-', 'immigrant', 'agenda', ',', '\"', 'the', 'texas', 'democrat', 'said', '.', '\"', 'that', 'means', 'we', \"'\", 've', 'been', 'in', 'the', 'news', 'a', 'lot', '.', '\"', 'es', '##co', '##bar', ',', 'one', 'of', 'two', 'of', 'the', 'first', 'latin', '##as', 'to', 'represent', 'texas', 'in', 'congress', ',', 'said', 'the', 'el', 'paso', 'community', \"'\", 's', 'response', 'to', 'a', 'months', '-', 'long', 'surge', 'of', 'migrants', '—', 'particularly', 'families', 'with', 'small', 'children', '—', 'heading', 'toward', 'the', 'southern', 'border', 'was', 'also', 'most', 'likely', 'targeted', 'by', 'the', 'gun', '##man', '.', 'she', 'said', 'she', 'herself', 'has', 'been', 'targeted', 'because', 'of', 'her', 'advocacy', 'in', 'this', 'field', '.', '\"', 'i', \"'\", 've', 'been', 'the', 'subject', 'of', 'death', 'threats', 'because', 'i', 'have', 'essentially', 'called', 'for', 'our', 'better', 'angels', 'during', 'these', 'challenging', 'times', ',', '\"', 'she', 'said', '.', '\"', 'i', \"'\", 've', 'been', 'attacked', 'for', 'asking', 'people', 'to', 'treat', 'migrants', 'with', 'compassion', '.', 'there', \"'\", 's', 'something', 'very', 'sick', 'happening', 'to', 'our', 'country', '.', '\"', 'located', 'in', 'one', 'of', 'the', 'most', 'highly', 'transit', '##ed', 'parts', 'of', 'the', 'u', '.', 's', '.', '-', 'mexico', 'border', ',', 'el', 'paso', 'has', 'fed', 'and', 'sheltered', 'tens', 'of', 'thousands', 'of', 'migrant', 'families', 'from', 'all', 'corners', 'of', 'the', 'globe', ',', 'particularly', 'central', 'america', ',', 'this', 'year', 'alone', '.', 'most', 'of', 'them', 'have', 'been', 'assisted', 'by', 'the', 'ann', '##unciation', 'house', 'network', 'of', 'shelters', ',', 'which', 'in', 'the', 'late', 'winter', 'and', 'early', 'spring', 'managed', 'to', 'house', '—', 'partly', 'by', 'spending', 'more', 'than', '$', '1', 'million', 'in', 'hotel', 'rooms', '—', 'thousands', 'of', 'migrant', 'families', 'released', 'by', 'immigration', 'and', 'customs', 'enforcement', '(', 'ice', ')', '.', 'the', 'ann', '##unciation', 'house', 'director', ',', 'ruben', 'garcia', ',', 'who', 'was', 'born', 'and', 'raised', 'in', 'the', 'city', ',', 'called', 'the', 'massacre', 'an', '\"', 'ob', '##sc', '##enity', '\"', 'and', 'stressed', 'that', 'the', 'alleged', 'shooter', ',', 'a', 'resident', 'of', 'the', 'dallas', 'suburb', 'of', 'allen', ',', 'is', 'not', 'a', 'member', 'of', 'the', 'el', 'paso', 'community', '.', '\"', 'this', 'is', 'someone', 'who', 'came', 'out', 'of', 'el', 'paso', 'to', 'do', 'harm', 'to', 'our', 'city', ',', 'to', 'our', 'people', ',', 'to', 'our', 'way', 'of', 'life', ',', '\"', 'garcia', 'told', 'cbs', 'news', 'in', 'an', 'interview', '.', '\"', 'and', 'i', 'think', 'first', ',', 'that', 'in', 'itself', 'is', 'an', 'acknowledge', '##ment', 'of', 'who', 'the', 'people', 'of', 'the', 'city', 'of', 'el', 'paso', 'are', '.', '\"', 'ann', '##unciation', 'house', 'director', 'ruben', 'garcia', ',', 'left', ',', 'stands', 'alongside', 'democratic', 'sen', '##s', '.', 'ma', '##zie', 'hi', '##ron', '##o', 'of', 'hawaii', 'and', 'tim', 'kai', '##ne', 'of', 'virginia', 'to', 'talk', 'to', 'a', 'migrant', 'family', 'at', 'the', '\"', 'casa', 'del', 'ref', '##ug', '##iad', '##o', '\"', 'shelter', 'in', 'el', 'paso', '.', 'cam', '##ilo', 'mont', '##oya', '-', 'gal', '##vez', 'like', 'es', '##co', '##bar', ',', 'the', 'shelter', 'director', 'described', 'el', 'paso', 'as', '\"', 'ground', 'zero', '\"', 'for', 'the', 'recent', 'large', 'influx', 'of', 'asylum', '-', 'seeking', 'families', 'and', 'the', 'administration', \"'\", 's', 'efforts', 'to', 'stem', 'their', 'migration', '.', 'he', 'noted', 'that', 'u', '.', 's', '.', 'officials', 'implemented', 'a', 'pilot', 'program', 'in', 'the', 'area', 'in', 'late', '2017', 'that', 'served', 'as', 'a', 'precursor', 'to', 'the', 'administration', \"'\", 's', 'controversial', '\"', 'zero', 'tolerance', '\"', 'policy', ',', 'which', 'led', 'to', 'the', 'for', '##ci', '##ble', 'separation', 'of', 'nearly', '3', ',', '000', 'migrant', 'families', '.', 'dylan', 'corbett', ',', 'executive', 'director', 'of', 'the', 'el', 'paso', '-', 'based', 'non', '-', 'profit', 'hope', 'border', 'institute', 'said', 'the', 'texas', 'border', 'city', \"'\", 's', 'response', 'to', 'the', 'unprecedented', 'flow', 'of', 'migrants', 'that', 'began', 'late', 'last', 'year', 'serves', 'as', 'a', 'stark', 'alternative', 'to', 'the', 'administration', \"'\", 's', 'deter', '##rence', 'efforts', '.', '\"', 'the', 'way', 'that', 'is', 'being', 'presented', 'to', 'us', 'by', 'the', 'people', 'in', 'power', 'in', 'the', 'white', 'house', 'is', 'that', 'we', 'have', 'respond', 'with', 'fear', '.', 'and', 'we', 'have', 'to', 'respond', 'with', 'demon', '##ization', '.', 'and', 'that', 'we', 'have', 'to', 'treat', 'these', 'people', 'like', 'criminals', '.', 'and', 'that', 'we', \"'\", 're', 'somehow', 'going', 'to', 'mil', '##ita', '##rize', 'to', 'solve', 'these', 'problems', '.', 'when', 'on', 'the', 'ground', ',', 'the', 'reality', 'here', 'has', 'been', 'quite', 'different', ',', '\"', 'corbett', 'told', 'cbs', 'news', 'in', 'an', 'interview', '.', 'dozens', 'of', 'murals', ',', 'including', 'the', 'one', 'above', ',', 'ad', '##orn', 'el', 'paso', ',', 'paying', 'tribute', 'to', 'the', 'city', \"'\", 's', 'diversity', 'and', 'ties', 'to', 'neighboring', 'mexico', '.', 'cam', '##ilo', 'mont', '##oya', '-', 'gal', '##vez', 'if', 'the', 'gun', '##man', 'targeted', 'el', 'paso', 'partly', 'because', 'of', 'the', 'community', \"'\", 's', 'advocacy', 'and', 'work', 'with', 'migrants', ',', 'corbett', 'said', ',', 'the', 'city', 'would', 'not', 'surrender', 'the', 'values', 'he', 'believes', 'defines', 'it', '.', '\"', 'if', 'we', \"'\", 've', 'been', 'attacked', 'because', 'of', 'that', ',', 'because', 'of', 'our', 'compassion', ',', 'because', 'of', 'generosity', ',', 'then', 'i', 'think', 'we', 'need', 'to', 'double', 'down', 'on', 'that', ',', '\"', 'he', 'said', '.', '\"', 'and', 'we', 'think', 'we', 'can', 'show', 'the', 'rest', 'for', 'the', 'country', 'that', 'these', 'values', 'are', 'worth', 'fighting', 'for', '.', '\"'] \n",
      "\n",
      "Token IDs:  [2043, 1037, 3282, 2386, 16201, 1037, 10789, 24547, 22345, 1999, 3449, 17161, 2006, 5095, 1010, 4288, 2012, 2560, 2322, 2111, 1998, 22736, 2062, 2084, 2048, 6474, 2500, 1010, 1996, 3146, 3675, 2103, 2001, 2718, 2007, 2019, 15741, 2504, 1997, 2668, 14740, 1998, 9940, 1012, 2247, 2007, 2178, 3742, 5008, 1999, 14700, 1010, 4058, 2070, 2410, 2847, 2101, 1010, 1996, 9288, 1999, 3449, 17161, 5853, 17572, 1996, 3811, 29308, 2120, 5981, 2105, 10340, 2000, 15176, 4409, 1012, 3449, 17161, 2003, 1037, 3675, 2451, 14477, 27631, 20389, 2098, 2000, 2107, 2312, 1011, 4094, 4490, 1997, 4808, 1012, 1998, 2005, 2116, 3901, 1997, 3449, 17161, 1517, 2019, 8680, 29110, 1997, 2178, 1997, 1996, 3842, 1005, 1055, 2087, 4487, 11365, 3512, 3314, 1010, 7521, 1517, 1996, 24665, 15808, 8462, 2886, 2025, 2069, 2104, 9363, 5596, 1996, 2342, 2000, 21573, 3229, 2000, 2152, 1011, 15977, 4255, 2066, 1996, 2028, 2109, 2011, 1996, 6884, 4632, 12502, 4630, 1010, 2009, 2036, 3421, 1037, 3154, 1998, 3622, 6101, 2006, 1996, 2103, 1005, 1055, 8906, 1998, 2049, 3061, 2004, 1037, 18066, 2451, 2005, 16836, 1012, 1000, 2057, 2428, 2123, 1005, 1056, 2031, 2000, 3984, 1012, 2057, 2113, 1012, 2057, 2020, 9416, 2138, 1996, 9452, 2359, 2000, 2886, 1037, 3262, 7402, 1998, 11560, 2451, 1010, 1000, 3537, 16360, 1012, 13133, 9686, 3597, 8237, 1010, 2040, 5836, 3449, 17161, 1010, 2409, 6568, 2739, 1999, 2019, 4357, 1012, 3449, 17161, 1010, 3009, 2005, 1000, 1996, 3413, 1010, 1000, 4832, 2012, 1996, 6840, 1997, 2048, 1057, 1012, 1055, 1012, 2163, 1010, 3146, 1998, 2047, 3290, 1010, 1998, 6661, 2019, 2248, 3675, 2007, 1996, 4916, 2103, 1997, 20759, 25398, 1012, 11503, 22360, 18318, 18232, 1011, 14891, 26132, 22704, 1999, 1996, 28480, 2078, 5532, 1010, 3449, 17161, 1010, 3009, 2005, 1000, 1996, 3413, 1010, 1000, 7719, 2012, 1996, 6840, 1997, 2048, 1057, 1012, 1055, 1012, 2163, 1010, 3146, 1998, 2047, 3290, 1010, 1998, 6661, 2019, 2248, 3675, 2007, 20759, 25398, 1010, 1996, 2922, 2103, 1999, 1996, 4916, 2110, 1997, 28480, 1012, 1999, 2023, 9197, 7402, 2451, 1010, 2188, 2000, 1037, 2312, 2177, 1997, 8026, 3370, 2389, 3667, 1998, 17636, 3901, 1010, 2116, 2449, 5751, 2024, 1999, 2119, 2394, 1998, 3009, 1998, 2155, 1011, 3079, 4916, 28496, 3111, 3233, 4077, 6700, 3334, 4157, 7340, 1012, 9686, 3597, 8237, 1998, 2060, 2451, 2372, 2903, 2037, 2103, 1005, 1055, 22050, 2004, 1037, 14400, 1997, 27135, 2964, 1998, 2844, 8026, 3370, 2389, 7208, 6296, 1996, 6878, 13108, 1010, 1037, 2538, 1011, 2095, 1011, 2214, 2317, 2158, 2040, 2003, 1999, 2231, 9968, 1012, 2116, 1997, 1996, 2235, 5661, 1999, 1996, 3653, 9527, 14776, 2135, 7402, 3675, 2103, 1997, 3449, 17161, 2031, 29404, 1999, 2119, 2394, 1998, 3009, 1012, 11503, 22360, 18318, 18232, 1011, 14891, 26132, 1996, 9252, 29216, 2003, 2108, 5845, 2004, 2019, 2552, 1997, 4968, 10130, 2011, 1996, 3425, 2533, 1998, 1037, 4022, 5223, 4126, 2011, 2976, 14766, 1010, 2040, 2024, 28664, 1037, 16939, 1010, 3424, 1011, 11560, 6254, 27023, 2135, 8786, 2011, 1996, 8343, 1012, 1996, 6884, 17124, 11703, 5134, 1996, 3652, 2576, 2373, 1997, 3146, 1005, 2312, 7402, 2451, 1998, 7939, 23709, 9623, 6555, 4460, 2006, 7521, 1012, 2066, 2060, 2152, 1011, 6337, 8037, 1010, 9686, 3597, 8237, 3881, 1037, 3622, 4957, 2090, 2343, 8398, 1005, 1055, 2524, 1011, 2240, 1998, 2411, 20187, 17871, 2006, 7521, 1998, 1996, 6835, 17108, 1997, 1996, 3282, 2386, 1012, 1999, 15443, 8320, 13867, 1998, 2880, 16413, 2015, 2000, 20807, 2010, 2524, 1011, 2240, 7521, 11376, 1010, 1996, 2343, 2038, 4846, 2616, 2066, 1000, 5274, 1000, 2000, 6235, 1996, 2929, 1997, 16836, 1012, 1000, 2057, 2031, 2042, 5720, 2055, 1037, 2843, 2011, 1996, 2343, 1012, 2057, 2031, 2042, 2598, 5717, 2005, 1996, 8398, 3447, 1005, 1055, 3424, 1011, 11560, 11376, 1010, 1000, 1996, 3146, 7672, 2056, 1012, 1000, 2008, 2965, 2057, 1005, 2310, 2042, 1999, 1996, 2739, 1037, 2843, 1012, 1000, 9686, 3597, 8237, 1010, 2028, 1997, 2048, 1997, 1996, 2034, 3763, 3022, 2000, 5050, 3146, 1999, 3519, 1010, 2056, 1996, 3449, 17161, 2451, 1005, 1055, 3433, 2000, 1037, 2706, 1011, 2146, 12058, 1997, 16836, 1517, 3391, 2945, 2007, 2235, 2336, 1517, 5825, 2646, 1996, 2670, 3675, 2001, 2036, 2087, 3497, 9416, 2011, 1996, 3282, 2386, 1012, 2016, 2056, 2016, 2841, 2038, 2042, 9416, 2138, 1997, 2014, 12288, 1999, 2023, 2492, 1012, 1000, 1045, 1005, 2310, 2042, 1996, 3395, 1997, 2331, 8767, 2138, 1045, 2031, 7687, 2170, 2005, 2256, 2488, 7048, 2076, 2122, 10368, 2335, 1010, 1000, 2016, 2056, 1012, 1000, 1045, 1005, 2310, 2042, 4457, 2005, 4851, 2111, 2000, 7438, 16836, 2007, 15398, 1012, 2045, 1005, 1055, 2242, 2200, 5305, 6230, 2000, 2256, 2406, 1012, 1000, 2284, 1999, 2028, 1997, 1996, 2087, 3811, 6671, 2098, 3033, 1997, 1996, 1057, 1012, 1055, 1012, 1011, 3290, 3675, 1010, 3449, 17161, 2038, 7349, 1998, 18304, 15295, 1997, 5190, 1997, 20731, 2945, 2013, 2035, 8413, 1997, 1996, 7595, 1010, 3391, 2430, 2637, 1010, 2023, 2095, 2894, 1012, 2087, 1997, 2068, 2031, 2042, 7197, 2011, 1996, 5754, 24101, 2160, 2897, 1997, 17177, 1010, 2029, 1999, 1996, 2397, 3467, 1998, 2220, 3500, 3266, 2000, 2160, 1517, 6576, 2011, 5938, 2062, 2084, 1002, 1015, 2454, 1999, 3309, 4734, 1517, 5190, 1997, 20731, 2945, 2207, 2011, 7521, 1998, 8205, 7285, 1006, 3256, 1007, 1012, 1996, 5754, 24101, 2160, 2472, 1010, 19469, 7439, 1010, 2040, 2001, 2141, 1998, 2992, 1999, 1996, 2103, 1010, 2170, 1996, 9288, 2019, 1000, 27885, 11020, 20693, 1000, 1998, 13233, 2008, 1996, 6884, 13108, 1010, 1037, 6319, 1997, 1996, 5759, 7575, 1997, 5297, 1010, 2003, 2025, 1037, 2266, 1997, 1996, 3449, 17161, 2451, 1012, 1000, 2023, 2003, 2619, 2040, 2234, 2041, 1997, 3449, 17161, 2000, 2079, 7386, 2000, 2256, 2103, 1010, 2000, 2256, 2111, 1010, 2000, 2256, 2126, 1997, 2166, 1010, 1000, 7439, 2409, 6568, 2739, 1999, 2019, 4357, 1012, 1000, 1998, 1045, 2228, 2034, 1010, 2008, 1999, 2993, 2003, 2019, 13399, 3672, 1997, 2040, 1996, 2111, 1997, 1996, 2103, 1997, 3449, 17161, 2024, 1012, 1000, 5754, 24101, 2160, 2472, 19469, 7439, 1010, 2187, 1010, 4832, 4077, 3537, 12411, 2015, 1012, 5003, 14272, 7632, 4948, 2080, 1997, 7359, 1998, 5199, 11928, 2638, 1997, 3448, 2000, 2831, 2000, 1037, 20731, 2155, 2012, 1996, 1000, 14124, 3972, 25416, 15916, 28665, 2080, 1000, 7713, 1999, 3449, 17161, 1012, 11503, 22360, 18318, 18232, 1011, 14891, 26132, 2066, 9686, 3597, 8237, 1010, 1996, 7713, 2472, 2649, 3449, 17161, 2004, 1000, 2598, 5717, 1000, 2005, 1996, 3522, 2312, 18050, 1997, 11386, 1011, 6224, 2945, 1998, 1996, 3447, 1005, 1055, 4073, 2000, 7872, 2037, 9230, 1012, 2002, 3264, 2008, 1057, 1012, 1055, 1012, 4584, 7528, 1037, 4405, 2565, 1999, 1996, 2181, 1999, 2397, 2418, 2008, 2366, 2004, 1037, 14988, 2000, 1996, 3447, 1005, 1055, 6801, 1000, 5717, 13986, 1000, 3343, 1010, 2029, 2419, 2000, 1996, 2005, 6895, 3468, 8745, 1997, 3053, 1017, 1010, 2199, 20731, 2945, 1012, 7758, 24119, 1010, 3237, 2472, 1997, 1996, 3449, 17161, 1011, 2241, 2512, 1011, 5618, 3246, 3675, 2820, 2056, 1996, 3146, 3675, 2103, 1005, 1055, 3433, 2000, 1996, 15741, 4834, 1997, 16836, 2008, 2211, 2397, 2197, 2095, 4240, 2004, 1037, 9762, 4522, 2000, 1996, 3447, 1005, 1055, 28283, 24413, 4073, 1012, 1000, 1996, 2126, 2008, 2003, 2108, 3591, 2000, 2149, 2011, 1996, 2111, 1999, 2373, 1999, 1996, 2317, 2160, 2003, 2008, 2057, 2031, 6869, 2007, 3571, 1012, 1998, 2057, 2031, 2000, 6869, 2007, 5698, 3989, 1012, 1998, 2008, 2057, 2031, 2000, 7438, 2122, 2111, 2066, 12290, 1012, 1998, 2008, 2057, 1005, 2128, 5064, 2183, 2000, 23689, 6590, 25709, 2000, 9611, 2122, 3471, 1012, 2043, 2006, 1996, 2598, 1010, 1996, 4507, 2182, 2038, 2042, 3243, 2367, 1010, 1000, 24119, 2409, 6568, 2739, 1999, 2019, 4357, 1012, 9877, 1997, 19016, 1010, 2164, 1996, 2028, 2682, 1010, 4748, 9691, 3449, 17161, 1010, 7079, 7050, 2000, 1996, 2103, 1005, 1055, 8906, 1998, 7208, 2000, 8581, 3290, 1012, 11503, 22360, 18318, 18232, 1011, 14891, 26132, 2065, 1996, 3282, 2386, 9416, 3449, 17161, 6576, 2138, 1997, 1996, 2451, 1005, 1055, 12288, 1998, 2147, 2007, 16836, 1010, 24119, 2056, 1010, 1996, 2103, 2052, 2025, 7806, 1996, 5300, 2002, 7164, 11859, 2009, 1012, 1000, 2065, 2057, 1005, 2310, 2042, 4457, 2138, 1997, 2008, 1010, 2138, 1997, 2256, 15398, 1010, 2138, 1997, 26161, 1010, 2059, 1045, 2228, 2057, 2342, 2000, 3313, 2091, 2006, 2008, 1010, 1000, 2002, 2056, 1012, 1000, 1998, 2057, 2228, 2057, 2064, 2265, 1996, 2717, 2005, 1996, 2406, 2008, 2122, 5300, 2024, 4276, 3554, 2005, 1012, 1000]\n",
      "Original Text:  Video\n",
      "Humans and robots working together in a factory may excite some tech geeks, but worry others who fear job losses.\n",
      "For Politics Live in Chester and Westminster, Jack Fenwick looks at the politics of automation, and what it could mean for both the white-collar and blue-collar employment markets.\n",
      "UK viewers can watch the full programme, including the debate that followed this film, for 30 days on iPlayer \n",
      "\n",
      "Tokenized Text:  ['video', 'humans', 'and', 'robots', 'working', 'together', 'in', 'a', 'factory', 'may', 'ex', '##cite', 'some', 'tech', 'geek', '##s', ',', 'but', 'worry', 'others', 'who', 'fear', 'job', 'losses', '.', 'for', 'politics', 'live', 'in', 'chester', 'and', 'westminster', ',', 'jack', 'fen', '##wick', 'looks', 'at', 'the', 'politics', 'of', 'automation', ',', 'and', 'what', 'it', 'could', 'mean', 'for', 'both', 'the', 'white', '-', 'collar', 'and', 'blue', '-', 'collar', 'employment', 'markets', '.', 'uk', 'viewers', 'can', 'watch', 'the', 'full', 'programme', ',', 'including', 'the', 'debate', 'that', 'followed', 'this', 'film', ',', 'for', '30', 'days', 'on', 'ip', '##layer'] \n",
      "\n",
      "Token IDs:  [2678, 4286, 1998, 13507, 2551, 2362, 1999, 1037, 4713, 2089, 4654, 17847, 2070, 6627, 29294, 2015, 1010, 2021, 4737, 2500, 2040, 3571, 3105, 6409, 1012, 2005, 4331, 2444, 1999, 8812, 1998, 9434, 1010, 2990, 21713, 7184, 3504, 2012, 1996, 4331, 1997, 19309, 1010, 1998, 2054, 2009, 2071, 2812, 2005, 2119, 1996, 2317, 1011, 9127, 1998, 2630, 1011, 9127, 6107, 6089, 1012, 2866, 7193, 2064, 3422, 1996, 2440, 4746, 1010, 2164, 1996, 5981, 2008, 2628, 2023, 2143, 1010, 2005, 2382, 2420, 2006, 12997, 24314]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix=True)\n",
    "\n",
    "for i in range(5):\n",
    "    print('Original Text: ', df['train']['headline'][i], '\\n')\n",
    "    print('Tokenized Text: ', tokenizer.tokenize(df['train']['headline'][i]), '\\n')\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['headline'][i])))\n",
    "\n",
    "for i in range(2):\n",
    "    print('Original Text: ', df['train']['body'][i], '\\n')\n",
    "    print('Tokenized Text: ', tokenizer.tokenize(df['train']['body'][i]), '\\n')\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['body'][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e',\n",
       " '##q',\n",
       " '##t',\n",
       " ',',\n",
       " 'k',\n",
       " '##kr',\n",
       " 'among',\n",
       " 'potential',\n",
       " 'bid',\n",
       " '##ders',\n",
       " 'for',\n",
       " 'long',\n",
       " 'beach',\n",
       " 'container',\n",
       " 'terminal']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('EQT, KKR Among Potential Bidders for Long Beach Container Terminal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vKd0MpK9BFPv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  Portage schools to use Sheriff's Department for resource officers in city \n",
      "\n",
      "Tokenized Text:  ['portage', 'schools', 'to', 'use', 'sheriff', \"'\", 's', 'department', 'for', 'resource', 'officers', 'in', 'city'] \n",
      "\n",
      "Token IDs:  [25140, 2816, 2000, 2224, 6458, 1005, 1055, 2533, 2005, 7692, 3738, 1999, 2103]\n",
      "Original Text:  EQT, KKR Among Potential Bidders for Long Beach Container Terminal \n",
      "\n",
      "Tokenized Text:  ['e', '##q', '##t', ',', 'k', '##kr', 'among', 'potential', 'bid', '##ders', 'for', 'long', 'beach', 'container', 'terminal'] \n",
      "\n",
      "Token IDs:  [1041, 4160, 2102, 1010, 1047, 21638, 2426, 4022, 7226, 13375, 2005, 2146, 3509, 11661, 5536]\n",
      "Original Text:  Kano: Meet Christians wey like carry dia case go Islamic police, Sharia court \n",
      "\n",
      "Tokenized Text:  ['kan', '##o', ':', 'meet', 'christians', 'we', '##y', 'like', 'carry', 'dia', 'case', 'go', 'islamic', 'police', ',', 'sha', '##ria', 'court'] \n",
      "\n",
      "Token IDs:  [22827, 2080, 1024, 3113, 8135, 2057, 2100, 2066, 4287, 22939, 2553, 2175, 5499, 2610, 1010, 21146, 4360, 2457]\n",
      "Original Text:  South Korea Regulator Says Samsung BioLogics Violated Accounting Rules \n",
      "\n",
      "Tokenized Text:  ['south', 'korea', 'regulator', 'says', 'samsung', 'bio', '##logic', '##s', 'violated', 'accounting', 'rules'] \n",
      "\n",
      "Token IDs:  [2148, 4420, 21618, 2758, 19102, 16012, 27179, 2015, 14424, 9529, 3513]\n",
      "Original Text:  U.S. grain handler Andersons eyes growth after Lansing deal \n",
      "\n",
      "Tokenized Text:  ['you', '.', 's', '.', 'grain', 'handler', 'anderson', '##s', 'eyes', 'growth', 'after', 'lansing', 'deal'] \n",
      "\n",
      "Token IDs:  [1057, 1012, 1055, 1012, 8982, 28213, 5143, 2015, 2159, 3930, 2044, 22304, 3066]\n",
      "Original Text:  The Portage Township Schools Corp. ended it’s nearly 20-year school resource officer relationship with the Portage Police department Friday.\n",
      "In a written statement, schools Superintendent Amanda Alaniz said the Porter County Sheriff’s office will take over the two full-time resource officer positions at Portage High School and the city’s two middle schools. The statement also says Portage police can apply for any of a number of “supplemental” resource officer positions that will work in the schools.\n",
      "The move blindsided Portage Police Chief Troy Williams, a former resource officer at the high school, who said he heard about his officers’ dismissals from the Post-Tribune. For nearly 20 years, two Portage police officers, one at the high school and one roving between the two middle schools, have served as full-time resource officers for Portage schools, with the school district and the city splitting the salaries for those officers.\n",
      "“I’m shocked, even given the events that have transpired up to now,” Williams said. “Summarily dismissing an entire (police) department after nearly 20 years to the Portage Township Schools is a disgrace.\n",
      "“But, with the relationships we’ve built with students and staff, you’re going to dismiss us at that cost?”\n",
      "In keeping with a board approved overhaul of its safety program, which also offered extensive mental health services for students and families, the school district also will have a number of part-time resource officer positions, paying about $30 an hour, for officers from a number of jurisdictions.\n",
      "Alaniz was unavailable for additional comment late Friday, but, in her statement, Alaniz described a “unified partnership with the Porter County Sheriff’s office.”\n",
      "She also said students and families should “expect these (resource officers) to be trained in school safety techniques, know our students, protect them, and defend them, should the need arise.”\n",
      "The move Friday, which was followed by a letter from an Indianapolis-based law firm Church Church Hittle and Antrim indicating any memorandum of understanding between the schools and the Portage Police Department has expired.\n",
      "The law firm also suggested potential legal action if “the City of Portage does not comply with the expectations set forth in this letter.”\n",
      "The Friday announcement is the latest salvo in what has been a running dispute between the Portage Police department, Portage Mayor James Snyder and the school district.\n",
      "In June, the board approved its comprehensive safety plan, which removed Portage police officers from the full-time slots, opening all resource officer work to police from different jurisdictions.\n",
      "Several days later, the school district reversed its vote on the resource officer portion of the comprehensive plan, keeping two full-time Portage officers in the schools and opening the supplemental positions to officers from other districts.\n",
      "Prior to the comprehensive plan vote, just days after a school shooting in Noblesville, Ind., Snyder and Williams offered two additional Portage police officers to work at the Portage schools for no additional cost to the schools.\n",
      "The district rejected the officer, and the schools and city officials have gone back-and-forth through e-mails, meetings and even a press conference by Alaniz and Deb Porter, president of the Portage teachers union.\n",
      "For weeks, Alaniz has said there is no working MOU between the schools and the city.\n",
      "Snyder pounced on Alaniz and the board after learning of the removal of the Portage police officers Friday, indicating Alaniz was “learning from the Opriskos on how to collaborate, and that’s ‘you do what we say or else’.”\n",
      "Snyder was referring to Portage City Council President Mark Oprisko, D- At large, and Cheryl Oprisko, vice president of the Portage Township school board.\n",
      "Williams has filed to run for Oprisko’s seat on the school board. According to Porter County Elections listings Friday, he will oppose Joan Machuca.\n",
      "“Superintendent Alaniz has made this issue about herself and not the kids,” Snyder said, adding he and Williams have made numerous “concessions” to get an agreement on the resource officer dispute with the school system.\n",
      "“I’d be very apprehensive about sending my kids to (Portage Township schools), because this is all about one person, Amanda Alaniz,” Snyder said.\n",
      "School Board President Andy Maletta, who also is the economic development director for the City of Portage, recused himself from any votes on the schools’ new safety plan. He could not be reached for comment Friday.\n",
      "Responding to Snyder’s comments, Alaniz said her move was “in alignment” with what the board approved in June, so she did not get any additional approval from the board to dismiss the Portage police officers.\n",
      "“And, as for Mayor Snyder’s comments about me, I think they’re telling of his character, not mine.”\n",
      "Michael Gonzalez is a freelance reporter for the Post-Tribune. \n",
      "\n",
      "Tokenized Text:  ['the', 'portage', 'township', 'schools', 'corp', '.', 'ended', 'it', 'is', 'nearly', '20', '-', 'year', 'school', 'resource', 'officer', 'relationship', 'with', 'the', 'portage', 'police', 'department', 'friday', '.', 'in', 'a', 'written', 'statement', ',', 'schools', 'superintendent', 'amanda', 'alan', '##iz', 'said', 'the', 'porter', 'county', 'sheriff', 's', 'office', 'will', 'take', 'over', 'the', 'two', 'full', '-', 'time', 'resource', 'officer', 'positions', 'at', 'portage', 'high', 'school', 'and', 'the', 'city', 's', 'two', 'middle', 'schools', '.', 'the', 'statement', 'also', 'says', 'portage', 'police', 'can', 'apply', 'for', 'any', 'of', 'a', 'number', 'of', 'supplemental', 'resource', 'officer', 'positions', 'that', 'will', 'work', 'in', 'the', 'schools', '.', 'the', 'move', 'blinds', '##ided', 'portage', 'police', 'chief', 'troy', 'williams', ',', 'a', 'former', 'resource', 'officer', 'at', 'the', 'high', 'school', ',', 'who', 'said', 'he', 'heard', 'about', 'his', 'officers', 'dismissal', '##s', 'from', 'the', 'post', '-', 'tribune', '.', 'for', 'nearly', '20', 'years', ',', 'two', 'portage', 'police', 'officers', ',', 'one', 'at', 'the', 'high', 'school', 'and', 'one', 'ro', '##ving', 'between', 'the', 'two', 'middle', 'schools', ',', 'have', 'served', 'as', 'full', '-', 'time', 'resource', 'officers', 'for', 'portage', 'schools', ',', 'with', 'the', 'school', 'district', 'and', 'the', 'city', 'splitting', 'the', 'salaries', 'for', 'those', 'officers', '.', 'i', 'am', 'shocked', ',', 'even', 'given', 'the', 'events', 'that', 'have', 'trans', '##pired', 'up', 'to', 'now', ',', 'williams', 'said', '.', 'sum', '##mar', '##ily', 'dismissing', 'an', 'entire', '(', 'police', ')', 'department', 'after', 'nearly', '20', 'years', 'to', 'the', 'portage', 'township', 'schools', 'is', 'a', 'disgrace', '.', 'but', ',', 'with', 'the', 'relationships', 'we', 'have', 'built', 'with', 'students', 'and', 'staff', ',', 'you', 'are', 'going', 'to', 'dismiss', 'us', 'at', 'that', 'cost', '?', 'in', 'keeping', 'with', 'a', 'board', 'approved', 'overhaul', 'of', 'its', 'safety', 'program', ',', 'which', 'also', 'offered', 'extensive', 'mental', 'health', 'services', 'for', 'students', 'and', 'families', ',', 'the', 'school', 'district', 'also', 'will', 'have', 'a', 'number', 'of', 'part', '-', 'time', 'resource', 'officer', 'positions', ',', 'paying', 'about', '$', '30', 'an', 'hour', ',', 'for', 'officers', 'from', 'a', 'number', 'of', 'jurisdictions', '.', 'alan', '##iz', 'was', 'unavailable', 'for', 'additional', 'comment', 'late', 'friday', ',', 'but', ',', 'in', 'her', 'statement', ',', 'alan', '##iz', 'described', 'a', 'unified', 'partnership', 'with', 'the', 'porter', 'county', 'sheriff', 's', 'office', '.', 'she', 'also', 'said', 'students', 'and', 'families', 'should', 'expect', 'these', '(', 'resource', 'officers', ')', 'to', 'be', 'trained', 'in', 'school', 'safety', 'techniques', ',', 'know', 'our', 'students', ',', 'protect', 'them', ',', 'and', 'defend', 'them', ',', 'should', 'the', 'need', 'arise', '.', 'the', 'move', 'friday', ',', 'which', 'was', 'followed', 'by', 'a', 'letter', 'from', 'an', 'indianapolis', '-', 'based', 'law', 'firm', 'church', 'church', 'hit', '##tle', 'and', 'antrim', 'indicating', 'any', 'memorandum', 'of', 'understanding', 'between', 'the', 'schools', 'and', 'the', 'portage', 'police', 'department', 'has', 'expired', '.', 'the', 'law', 'firm', 'also', 'suggested', 'potential', 'legal', 'action', 'if', 'the', 'city', 'of', 'portage', 'does', 'not', 'comply', 'with', 'the', 'expectations', 'set', 'forth', 'in', 'this', 'letter', '.', 'the', 'friday', 'announcement', 'is', 'the', 'latest', 'sal', '##vo', 'in', 'what', 'has', 'been', 'a', 'running', 'dispute', 'between', 'the', 'portage', 'police', 'department', ',', 'portage', 'mayor', 'james', 'snyder', 'and', 'the', 'school', 'district', '.', 'in', 'june', ',', 'the', 'board', 'approved', 'its', 'comprehensive', 'safety', 'plan', ',', 'which', 'removed', 'portage', 'police', 'officers', 'from', 'the', 'full', '-', 'time', 'slots', ',', 'opening', 'all', 'resource', 'officer', 'work', 'to', 'police', 'from', 'different', 'jurisdictions', '.', 'several', 'days', 'later', ',', 'the', 'school', 'district', 'reversed', 'its', 'vote', 'on', 'the', 'resource', 'officer', 'portion', 'of', 'the', 'comprehensive', 'plan', ',', 'keeping', 'two', 'full', '-', 'time', 'portage', 'officers', 'in', 'the', 'schools', 'and', 'opening', 'the', 'supplemental', 'positions', 'to', 'officers', 'from', 'other', 'districts', '.', 'prior', 'to', 'the', 'comprehensive', 'plan', 'vote', ',', 'just', 'days', 'after', 'a', 'school', 'shooting', 'in', 'nobles', '##ville', ',', 'ind', '.', ',', 'snyder', 'and', 'williams', 'offered', 'two', 'additional', 'portage', 'police', 'officers', 'to', 'work', 'at', 'the', 'portage', 'schools', 'for', 'no', 'additional', 'cost', 'to', 'the', 'schools', '.', 'the', 'district', 'rejected', 'the', 'officer', ',', 'and', 'the', 'schools', 'and', 'city', 'officials', 'have', 'gone', 'back', '-', 'and', '-', 'forth', 'through', 'e', '-', 'mail', '##s', ',', 'meetings', 'and', 'even', 'a', 'press', 'conference', 'by', 'alan', '##iz', 'and', 'de', '##b', 'porter', ',', 'president', 'of', 'the', 'portage', 'teachers', 'union', '.', 'for', 'weeks', ',', 'alan', '##iz', 'has', 'said', 'there', 'is', 'no', 'working', 'mo', '##u', 'between', 'the', 'schools', 'and', 'the', 'city', '.', 'snyder', 'po', '##unced', 'on', 'alan', '##iz', 'and', 'the', 'board', 'after', 'learning', 'of', 'the', 'removal', 'of', 'the', 'portage', 'police', 'officers', 'friday', ',', 'indicating', 'alan', '##iz', 'was', 'learning', 'from', 'the', 'op', '##ris', '##kos', 'on', 'how', 'to', 'collaborate', ',', 'and', 'that', 'is', 'you', 'do', 'what', 'we', 'say', 'or', 'else', '.', 'snyder', 'was', 'referring', 'to', 'portage', 'city', 'council', 'president', 'mark', 'op', '##ris', '##ko', ',', 'd', '-', 'at', 'large', ',', 'and', 'cheryl', 'op', '##ris', '##ko', ',', 'vice', 'president', 'of', 'the', 'portage', 'township', 'school', 'board', '.', 'williams', 'has', 'filed', 'to', 'run', 'for', 'op', '##ris', '##ko', 's', 'seat', 'on', 'the', 'school', 'board', '.', 'according', 'to', 'porter', 'county', 'elections', 'listings', 'friday', ',', 'he', 'will', 'oppose', 'joan', 'mach', '##uca', '.', 'superintendent', 'alan', '##iz', 'has', 'made', 'this', 'issue', 'about', 'herself', 'and', 'not', 'the', 'kids', ',', 'snyder', 'said', ',', 'adding', 'he', 'and', 'williams', 'have', 'made', 'numerous', 'concessions', 'to', 'get', 'an', 'agreement', 'on', 'the', 'resource', 'officer', 'dispute', 'with', 'the', 'school', 'system', '.', 'i', 'would', 'be', 'very', 'app', '##re', '##hen', '##sive', 'about', 'sending', 'my', 'kids', 'to', '(', 'portage', 'township', 'schools', ')', ',', 'because', 'this', 'is', 'all', 'about', 'one', 'person', ',', 'amanda', 'alan', '##iz', ',', 'snyder', 'said', '.', 'school', 'board', 'president', 'andy', 'male', '##tta', ',', 'who', 'also', 'is', 'the', 'economic', 'development', 'director', 'for', 'the', 'city', 'of', 'portage', ',', 'rec', '##used', 'himself', 'from', 'any', 'votes', 'on', 'the', 'schools', 'new', 'safety', 'plan', '.', 'he', 'could', 'not', 'be', 'reached', 'for', 'comment', 'friday', '.', 'responding', 'to', 'snyder', 's', 'comments', ',', 'alan', '##iz', 'said', 'her', 'move', 'was', 'in', 'alignment', 'with', 'what', 'the', 'board', 'approved', 'in', 'june', ',', 'so', 'she', 'did', 'not', 'get', 'any', 'additional', 'approval', 'from', 'the', 'board', 'to', 'dismiss', 'the', 'portage', 'police', 'officers', '.', 'and', ',', 'as', 'for', 'mayor', 'snyder', 's', 'comments', 'about', 'me', ',', 'i', 'think', 'they', 'are', 'telling', 'of', 'his', 'character', ',', 'not', 'mine', '.', 'michael', 'gonzalez', 'is', 'a', 'freelance', 'reporter', 'for', 'the', 'post', '-', 'tribune', '.'] \n",
      "\n",
      "Token IDs:  [1996, 25140, 3545, 2816, 13058, 1012, 3092, 2009, 1521, 1055, 3053, 2322, 1011, 2095, 2082, 7692, 2961, 3276, 2007, 1996, 25140, 2610, 2533, 5958, 1012, 1999, 1037, 2517, 4861, 1010, 2816, 9133, 8282, 5070, 10993, 2056, 1996, 8716, 2221, 6458, 1521, 1055, 2436, 2097, 2202, 2058, 1996, 2048, 2440, 1011, 2051, 7692, 2961, 4460, 2012, 25140, 2152, 2082, 1998, 1996, 2103, 1521, 1055, 2048, 2690, 2816, 1012, 1996, 4861, 2036, 2758, 25140, 2610, 2064, 6611, 2005, 2151, 1997, 1037, 2193, 1997, 1523, 27024, 1524, 7692, 2961, 4460, 2008, 2097, 2147, 1999, 1996, 2816, 1012, 1996, 2693, 28279, 14097, 25140, 2610, 2708, 9553, 3766, 1010, 1037, 2280, 7692, 2961, 2012, 1996, 2152, 2082, 1010, 2040, 2056, 2002, 2657, 2055, 2010, 3738, 1521, 15322, 2015, 2013, 1996, 2695, 1011, 10969, 1012, 2005, 3053, 2322, 2086, 1010, 2048, 25140, 2610, 3738, 1010, 2028, 2012, 1996, 2152, 2082, 1998, 2028, 20996, 6455, 2090, 1996, 2048, 2690, 2816, 1010, 2031, 2366, 2004, 2440, 1011, 2051, 7692, 3738, 2005, 25140, 2816, 1010, 2007, 1996, 2082, 2212, 1998, 1996, 2103, 14541, 1996, 20566, 2005, 2216, 3738, 1012, 1523, 1045, 1521, 1049, 7135, 1010, 2130, 2445, 1996, 2824, 2008, 2031, 9099, 21649, 2039, 2000, 2085, 1010, 1524, 3766, 2056, 1012, 1523, 7680, 7849, 6588, 28913, 2019, 2972, 1006, 2610, 1007, 2533, 2044, 3053, 2322, 2086, 2000, 1996, 25140, 3545, 2816, 2003, 1037, 29591, 1012, 1523, 2021, 1010, 2007, 1996, 6550, 2057, 1521, 2310, 2328, 2007, 2493, 1998, 3095, 1010, 2017, 1521, 2128, 2183, 2000, 19776, 2149, 2012, 2008, 3465, 1029, 1524, 1999, 4363, 2007, 1037, 2604, 4844, 18181, 1997, 2049, 3808, 2565, 1010, 2029, 2036, 3253, 4866, 5177, 2740, 2578, 2005, 2493, 1998, 2945, 1010, 1996, 2082, 2212, 2036, 2097, 2031, 1037, 2193, 1997, 2112, 1011, 2051, 7692, 2961, 4460, 1010, 7079, 2055, 1002, 2382, 2019, 3178, 1010, 2005, 3738, 2013, 1037, 2193, 1997, 17370, 1012, 5070, 10993, 2001, 20165, 2005, 3176, 7615, 2397, 5958, 1010, 2021, 1010, 1999, 2014, 4861, 1010, 5070, 10993, 2649, 1037, 1523, 10562, 5386, 2007, 1996, 8716, 2221, 6458, 1521, 1055, 2436, 1012, 1524, 2016, 2036, 2056, 2493, 1998, 2945, 2323, 1523, 5987, 2122, 1006, 7692, 3738, 1007, 2000, 2022, 4738, 1999, 2082, 3808, 5461, 1010, 2113, 2256, 2493, 1010, 4047, 2068, 1010, 1998, 6985, 2068, 1010, 2323, 1996, 2342, 13368, 1012, 1524, 1996, 2693, 5958, 1010, 2029, 2001, 2628, 2011, 1037, 3661, 2013, 2019, 9506, 1011, 2241, 2375, 3813, 2277, 2277, 2718, 9286, 1998, 24142, 8131, 2151, 20336, 1997, 4824, 2090, 1996, 2816, 1998, 1996, 25140, 2610, 2533, 2038, 13735, 1012, 1996, 2375, 3813, 2036, 4081, 4022, 3423, 2895, 2065, 1523, 1996, 2103, 1997, 25140, 2515, 2025, 14037, 2007, 1996, 10908, 2275, 5743, 1999, 2023, 3661, 1012, 1524, 1996, 5958, 8874, 2003, 1996, 6745, 16183, 6767, 1999, 2054, 2038, 2042, 1037, 2770, 7593, 2090, 1996, 25140, 2610, 2533, 1010, 25140, 3664, 2508, 17840, 1998, 1996, 2082, 2212, 1012, 1999, 2238, 1010, 1996, 2604, 4844, 2049, 7721, 3808, 2933, 1010, 2029, 3718, 25140, 2610, 3738, 2013, 1996, 2440, 1011, 2051, 19832, 1010, 3098, 2035, 7692, 2961, 2147, 2000, 2610, 2013, 2367, 17370, 1012, 2195, 2420, 2101, 1010, 1996, 2082, 2212, 11674, 2049, 3789, 2006, 1996, 7692, 2961, 4664, 1997, 1996, 7721, 2933, 1010, 4363, 2048, 2440, 1011, 2051, 25140, 3738, 1999, 1996, 2816, 1998, 3098, 1996, 27024, 4460, 2000, 3738, 2013, 2060, 4733, 1012, 3188, 2000, 1996, 7721, 2933, 3789, 1010, 2074, 2420, 2044, 1037, 2082, 5008, 1999, 13969, 3077, 1010, 27427, 1012, 1010, 17840, 1998, 3766, 3253, 2048, 3176, 25140, 2610, 3738, 2000, 2147, 2012, 1996, 25140, 2816, 2005, 2053, 3176, 3465, 2000, 1996, 2816, 1012, 1996, 2212, 5837, 1996, 2961, 1010, 1998, 1996, 2816, 1998, 2103, 4584, 2031, 2908, 2067, 1011, 1998, 1011, 5743, 2083, 1041, 1011, 5653, 2015, 1010, 6295, 1998, 2130, 1037, 2811, 3034, 2011, 5070, 10993, 1998, 2139, 2497, 8716, 1010, 2343, 1997, 1996, 25140, 5089, 2586, 1012, 2005, 3134, 1010, 5070, 10993, 2038, 2056, 2045, 2003, 2053, 2551, 9587, 2226, 2090, 1996, 2816, 1998, 1996, 2103, 1012, 17840, 13433, 22392, 2006, 5070, 10993, 1998, 1996, 2604, 2044, 4083, 1997, 1996, 8208, 1997, 1996, 25140, 2610, 3738, 5958, 1010, 8131, 5070, 10993, 2001, 1523, 4083, 2013, 1996, 6728, 6935, 15710, 2006, 2129, 2000, 20880, 1010, 1998, 2008, 1521, 1055, 1520, 2017, 2079, 2054, 2057, 2360, 2030, 2842, 1521, 1012, 1524, 17840, 2001, 7727, 2000, 25140, 2103, 2473, 2343, 2928, 6728, 6935, 3683, 1010, 1040, 1011, 2012, 2312, 1010, 1998, 19431, 6728, 6935, 3683, 1010, 3580, 2343, 1997, 1996, 25140, 3545, 2082, 2604, 1012, 3766, 2038, 6406, 2000, 2448, 2005, 6728, 6935, 3683, 1521, 1055, 2835, 2006, 1996, 2082, 2604, 1012, 2429, 2000, 8716, 2221, 3864, 26213, 5958, 1010, 2002, 2097, 15391, 7437, 24532, 18100, 1012, 1523, 9133, 5070, 10993, 2038, 2081, 2023, 3277, 2055, 2841, 1998, 2025, 1996, 4268, 1010, 1524, 17840, 2056, 1010, 5815, 2002, 1998, 3766, 2031, 2081, 3365, 1523, 20638, 1524, 2000, 2131, 2019, 3820, 2006, 1996, 7692, 2961, 7593, 2007, 1996, 2082, 2291, 1012, 1523, 1045, 1521, 1040, 2022, 2200, 10439, 2890, 10222, 12742, 2055, 6016, 2026, 4268, 2000, 1006, 25140, 3545, 2816, 1007, 1010, 2138, 2023, 2003, 2035, 2055, 2028, 2711, 1010, 8282, 5070, 10993, 1010, 1524, 17840, 2056, 1012, 2082, 2604, 2343, 5557, 3287, 5946, 1010, 2040, 2036, 2003, 1996, 3171, 2458, 2472, 2005, 1996, 2103, 1997, 25140, 1010, 28667, 13901, 2370, 2013, 2151, 4494, 2006, 1996, 2816, 1521, 2047, 3808, 2933, 1012, 2002, 2071, 2025, 2022, 2584, 2005, 7615, 5958, 1012, 14120, 2000, 17840, 1521, 1055, 7928, 1010, 5070, 10993, 2056, 2014, 2693, 2001, 1523, 1999, 12139, 1524, 2007, 2054, 1996, 2604, 4844, 1999, 2238, 1010, 2061, 2016, 2106, 2025, 2131, 2151, 3176, 6226, 2013, 1996, 2604, 2000, 19776, 1996, 25140, 2610, 3738, 1012, 1523, 1998, 1010, 2004, 2005, 3664, 17840, 1521, 1055, 7928, 2055, 2033, 1010, 1045, 2228, 2027, 1521, 2128, 4129, 1997, 2010, 2839, 1010, 2025, 3067, 1012, 1524, 2745, 10121, 2003, 1037, 15919, 6398, 2005, 1996, 2695, 1011, 10969, 1012]\n",
      "Original Text:  Global investors including EQT Partners and KKR & Co. are looking at bidding for the Long Beach Container Terminal in Southern California, as are seasoned ship operators Seaspan Corp. and Hyundai Merchant Marine, according to people directly involved in the sale.\n",
      "The sale of the major gateway for U.S. seaborne trade with China is expected to fetch as much as $2 billion and comes as its owner—China’s Cosco Shipping Holdings Co.—looks to dispose of the terminal by the first half of next year.\n",
      "... \n",
      "\n",
      "Tokenized Text:  ['global', 'investors', 'including', 'e', '##q', '##t', 'partners', 'and', 'k', '##kr', '&', 'co', '.', 'are', 'looking', 'at', 'bidding', 'for', 'the', 'long', 'beach', 'container', 'terminal', 'in', 'southern', 'california', ',', 'as', 'are', 'seasoned', 'ship', 'operators', 'seas', '##pan', 'corp', '.', 'and', 'hyundai', 'merchant', 'marine', ',', 'according', 'to', 'people', 'directly', 'involved', 'in', 'the', 'sale', '.', 'the', 'sale', 'of', 'the', 'major', 'gateway', 'for', 'you', '.', 's', '.', 'sea', '##borne', 'trade', 'with', 'china', 'is', 'expected', 'to', 'fetch', 'as', 'much', 'as', '$', '2', 'billion', 'and', 'comes', 'as', 'its', 'owner', 'china', 's', 'co', '##sco', 'shipping', 'holdings', 'co', '.', 'looks', 'to', 'dispose', 'of', 'the', 'terminal', 'by', 'the', 'first', 'half', 'of', 'next', 'year', '.', '.', '.', '.'] \n",
      "\n",
      "Token IDs:  [3795, 9387, 2164, 1041, 4160, 2102, 5826, 1998, 1047, 21638, 1004, 2522, 1012, 2024, 2559, 2012, 17534, 2005, 1996, 2146, 3509, 11661, 5536, 1999, 2670, 2662, 1010, 2004, 2024, 28223, 2911, 9224, 11915, 9739, 13058, 1012, 1998, 25983, 6432, 3884, 1010, 2429, 2000, 2111, 3495, 2920, 1999, 1996, 5096, 1012, 1996, 5096, 1997, 1996, 2350, 11909, 2005, 1057, 1012, 1055, 1012, 2712, 19288, 3119, 2007, 2859, 2003, 3517, 2000, 18584, 2004, 2172, 2004, 1002, 1016, 4551, 1998, 3310, 2004, 2049, 3954, 1517, 2859, 1521, 1055, 2522, 9363, 7829, 9583, 2522, 1012, 1517, 3504, 2000, 27764, 1997, 1996, 5536, 2011, 1996, 2034, 2431, 1997, 2279, 2095, 1012, 1012, 1012, 1012]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#lemmatization and removing stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    def is_english_word(word):\n",
    "        \"\"\"Function to filter out non-English words.\"\"\"\n",
    "        return bool(re.match(r'^[a-zA-Z]+$', word))\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    # Remove non-English characters and punctuation\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Removes non-ASCII characters\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print('Original Text: ', df['train']['headline'][i], '\\n')\n",
    "    print('Tokenized Text: ', tokenizer.tokenize(preprocess(df['train']['headline'][i])), '\\n')\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['headline'][i])))\n",
    "\n",
    "for i in range(2):\n",
    "    print('Original Text: ', df['train']['body'][i], '\\n')\n",
    "    print('Tokenized Text: ', tokenizer.tokenize(preprocess(df['train']['body'][i])), '\\n')\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['body'][i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ERySJZcQBp9_"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    text = examples[\"body\"]\n",
    "    labels = examples[\"political_leaning\"]  \n",
    "    \n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors = \"np\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 512\n",
    "        )\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = [label2id[label] for label in labels]  \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "37\n",
      "0\n",
      "15\n",
      "12036\n",
      "29515\n"
     ]
    }
   ],
   "source": [
    "texts = df['train']['headline']\n",
    "\n",
    "# Handle None or missing values by filtering out None entries\n",
    "text_lengths = [len(text.split(' ')) if text is not None else 0 for text in texts]\n",
    "\n",
    "print(min(text_lengths))\n",
    "print(max(text_lengths))\n",
    "\n",
    "# Count how many texts have 300 or more words\n",
    "print(sum([1 for length in text_lengths if length >= 300]))\n",
    "\n",
    "# Repeat for the 'body' column\n",
    "texts = df['train']['body']\n",
    "\n",
    "# Handle None or missing values by filtering out None entries\n",
    "text_lengths = [len(text.split()) if text is not None else 0 for text in texts]\n",
    "\n",
    "print(min(text_lengths))\n",
    "print(max(text_lengths))\n",
    "\n",
    "# Count how many texts have 300 or more words\n",
    "print(sum([1 for length in text_lengths if length >= 300]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "YmGXgdNQCb7e"
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "  model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ZgXdBTYUDLz6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 136735/136735 [01:55<00:00, 1179.22 examples/s]\n",
      "Map: 100%|██████████| 15193/15193 [00:14<00:00, 1015.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 136735\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15193\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = df.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IjSOWH6wDNHw"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KJvaBgtkD963"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "  predictions, labels = p\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return {\"accuracy\": accuracy.compute(predictions=predictions\n",
    "                                       , references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8xiaVnUaF1Yf"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type='SEQ_CLS',\n",
    "                         r = 4,\n",
    "                         lora_alpha=32,\n",
    "                         lora_dropout=0.01,\n",
    "                         target_modules = ['q_lin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "o4hduUwTGnN5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 630,532 || all params: 67,587,080 || trainable%: 0.9329\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=4, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.distilbert.embeddings.word_embeddings.weight: requires_grad=False\n",
      "base_model.model.distilbert.embeddings.position_embeddings.weight: requires_grad=False\n",
      "base_model.model.distilbert.embeddings.LayerNorm.weight: requires_grad=False\n",
      "base_model.model.distilbert.embeddings.LayerNorm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.pre_classifier.original_module.weight: requires_grad=False\n",
      "base_model.model.pre_classifier.original_module.bias: requires_grad=False\n",
      "base_model.model.pre_classifier.modules_to_save.default.weight: requires_grad=True\n",
      "base_model.model.pre_classifier.modules_to_save.default.bias: requires_grad=True\n",
      "base_model.model.classifier.original_module.weight: requires_grad=False\n",
      "base_model.model.classifier.original_module.bias: requires_grad=False\n",
      "base_model.model.classifier.modules_to_save.default.weight: requires_grad=True\n",
      "base_model.model.classifier.modules_to_save.default.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ORBVjXnGx19"
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"\"+model_checkpoint+\"lora-txt\",\n",
    "    learning_rate = lr,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    num_train_epochs = num_epochs,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "hmS4TS65IDDV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/r66735gn7xx2tdk0357s_k_m0000gn/T/ipykernel_86100/1961861285.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    eval_dataset = tokenized_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfzk-YnMJL0V"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='235' max='66025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  235/66025 01:50 < 8:39:20, 2.11 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ALEJANDRO\\AppData\\Local\\Temp\\ipykernel_21772\\2209749692.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"C:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\models\\LORA_distilBERT_BODY_2017_1.pth\", map_location=device), strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Recreate LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\", r=4, lora_alpha=32, lora_dropout=0.01, target_modules=[\"q_lin\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=4, id2label=id2label, label2id=label2id)\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "# Load LoRA adapter weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\models\\LORA_distilBERT_BODY_2017_1.pth\", map_location=device), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jn1iHMyJNtM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALEJANDRO\\AppData\\Local\\Temp\\ipykernel_21772\\3700068903.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"LORA_distilBERT_BODY_2017_1.pth\", map_location=device)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LORA_distilBERT_BODY_2017_1.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLORA_distilBERT_BODY_2017_1.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m text_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mPresident-elect Trump announced on Tuesday night that he intends to appoint Linda McMahon, former CEO of World Wrestling Entertainment (WWE), to lead the Department of Education. His announcement, which was posted on Truth Social, came hours after two sources told Fox News that McMahon was likely to be picked. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is my great honor to announce that Linda McMahon, former Administrator of the Small Business Administration, will be the United States Secretary of Education,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Trump\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms statement read.\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs Secretary of Education, Linda will fight tirelessly to expand Choice to every State in America, and empower parents to make the best Education decisions for their families,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the press release added. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinda served for two years on the Connecticut Board of Education, where she was one of fifteen members overseeing all Public Education in the State, including its Technical High School system.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'''\u001b[39m, \n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m             \u001b[39m\u001b[38;5;124;03m'''Donald Trump believes presidents have almost absolute power. In his second term, there will be few political or legal restraints to check him. The president-elects sweeping victory over Vice President Kamala Harris suddenly turned the theoretical notion that he will indulge his autocratic instincts into a genuine possibility.When Trump returns to the White House in January as one of the most powerful presidents in history, hell be able to take advantage of his own filleting of guardrails during his first presidency, which he continued through legal maneuverings out of office.'''\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mVoters in nine states, including South Dakota, rejected abortion rights measures during the November election. \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     12\u001b[0m              ]\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LORA_distilBERT_BODY_2017_1.pth'"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dict = torch.load(\"trained_model_gral_imbd.pth\", map_location=device)\n",
    "\n",
    "text_list = ['''President-elect Trump announced on Tuesday night that he intends to appoint Linda McMahon, former CEO of World Wrestling Entertainment (WWE), to lead the Department of Education. His announcement, which was posted on Truth Social, came hours after two sources told Fox News that McMahon was likely to be picked. \"It is my great honor to announce that Linda McMahon, former Administrator of the Small Business Administration, will be the United States Secretary of Education,\" Trump's statement read.\n",
    "\"As Secretary of Education, Linda will fight tirelessly to expand Choice to every State in America, and empower parents to make the best Education decisions for their families,\" the press release added. \"Linda served for two years on the Connecticut Board of Education, where she was one of fifteen members overseeing all Public Education in the State, including its Technical High School system.\"''', \n",
    "             '''Donald Trump believes presidents have almost absolute power. In his second term, there will be few political or legal restraints to check him. The president-elects sweeping victory over Vice President Kamala Harris suddenly turned the theoretical notion that he will indulge his autocratic instincts into a genuine possibility.When Trump returns to the White House in January as one of the most powerful presidents in history, hell be able to take advantage of his own filleting of guardrails during his first presidency, which he continued through legal maneuverings out of office.''',\n",
    "             '''Nearly 100 Democrats, including Salud Carbajal, requested the Ethics Committee release its report on former Congressman Matt Gaetz's misconduct allegations. The letter, led by Rep. Sean Casten, emphasized that the Senate needs information for Gaetz's attorney general nomination. House Speaker Mike Johnson opposed releasing the report, stating Gaetz is now a \"private citizen\" and outside the panel's jurisdiction.'''\n",
    "             , ''' A South Dakota judge dismissed a lawsuit from the anti-abortion group Life Defense targeting an abortion rights measure that voters later rejected.\n",
    "Judge John Pekas dismissed the lawsuit at the request of Life Defense, which had challenged the ballot measure's petitions.\n",
    "Voters in nine states, including South Dakota, rejected abortion rights measures during the November election. '''\n",
    "             ]\n",
    "model.to('cuda')\n",
    "print('Trained model predictions')\n",
    "for text in text_list:\n",
    "  inputs = tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
    "\n",
    "  logits = model(inputs).logits\n",
    "  predictions = torch.max(logits,1).indices\n",
    "\n",
    "  #print(f'{text} - {id2label[predictions.tolist()[0]]}')\n",
    "  print(f'{id2label[predictions.tolist()[0]]}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_lda_implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
