{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alejandro Paredes, Parameter tuning of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwAV24NJ-ZSj",
    "outputId": "f707f622-fcd8-4645-fc3f-6beaef291c8d"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets peft evaluate #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C2gg1Syx-s44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KfUh_vrS_hbR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "#Define label maps\n",
    "id2label = {0:\"UNDEFINED\" ,1:\"LEFT\",2:\"RIGHT\",3:\"CENTER\"}\n",
    "label2id = {\"UNDEFINED\": 0, \"LEFT\": 1, \"RIGHT\": 2, \"CENTER\": 3}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=4, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yrj-hSULAi_a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "        num_rows: 168809\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df = load_dataset(\"csv\", data_files=\"./data/2018_2.csv\", split=\"train\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "        num_rows: 136735\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "        num_rows: 15193\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_testvalid = \n",
    "#df = df['train'].train_test_split(test_size=0.1)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix=True)\n",
    "\n",
    "for i in range(5):\n",
    "    print('Original Text: ', df['train']['headline'][i], '\\n')\n",
    "    print('Tokenized Text: ', tokenizer.tokenize(df['train']['headline'][i]), '\\n')\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['headline'][i])))\n",
    "\n",
    "for i in range(2):\n",
    "    print('Original Text: ', df['train']['body'][i], '\\n')\n",
    "    print('Tokenized Text: ', tokenizer.tokenize(df['train']['body'][i]), '\\n')\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['body'][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e',\n",
       " '##q',\n",
       " '##t',\n",
       " ',',\n",
       " 'k',\n",
       " '##kr',\n",
       " 'among',\n",
       " 'potential',\n",
       " 'bid',\n",
       " '##ders',\n",
       " 'for',\n",
       " 'long',\n",
       " 'beach',\n",
       " 'container',\n",
       " 'terminal']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('EQT, KKR Among Potential Bidders for Long Beach Container Terminal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vKd0MpK9BFPv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  Portage schools to use Sheriff's Department for resource officers in city \n",
      "\n",
      "Tokenized Text:  ['portage', 'schools', 'to', 'use', 'sheriff', \"'\", 's', 'department', 'for', 'resource', 'officers', 'in', 'city'] \n",
      "\n",
      "Token IDs:  [25140, 2816, 2000, 2224, 6458, 1005, 1055, 2533, 2005, 7692, 3738, 1999, 2103]\n",
      "Original Text:  EQT, KKR Among Potential Bidders for Long Beach Container Terminal \n",
      "\n",
      "Tokenized Text:  ['e', '##q', '##t', ',', 'k', '##kr', 'among', 'potential', 'bid', '##ders', 'for', 'long', 'beach', 'container', 'terminal'] \n",
      "\n",
      "Token IDs:  [1041, 4160, 2102, 1010, 1047, 21638, 2426, 4022, 7226, 13375, 2005, 2146, 3509, 11661, 5536]\n",
      "Original Text:  Kano: Meet Christians wey like carry dia case go Islamic police, Sharia court \n",
      "\n",
      "Tokenized Text:  ['kan', '##o', ':', 'meet', 'christians', 'we', '##y', 'like', 'carry', 'dia', 'case', 'go', 'islamic', 'police', ',', 'sha', '##ria', 'court'] \n",
      "\n",
      "Token IDs:  [22827, 2080, 1024, 3113, 8135, 2057, 2100, 2066, 4287, 22939, 2553, 2175, 5499, 2610, 1010, 21146, 4360, 2457]\n",
      "Original Text:  South Korea Regulator Says Samsung BioLogics Violated Accounting Rules \n",
      "\n",
      "Tokenized Text:  ['south', 'korea', 'regulator', 'says', 'samsung', 'bio', '##logic', '##s', 'violated', 'accounting', 'rules'] \n",
      "\n",
      "Token IDs:  [2148, 4420, 21618, 2758, 19102, 16012, 27179, 2015, 14424, 9529, 3513]\n",
      "Original Text:  U.S. grain handler Andersons eyes growth after Lansing deal \n",
      "\n",
      "Tokenized Text:  ['you', '.', 's', '.', 'grain', 'handler', 'anderson', '##s', 'eyes', 'growth', 'after', 'lansing', 'deal'] \n",
      "\n",
      "Token IDs:  [1057, 1012, 1055, 1012, 8982, 28213, 5143, 2015, 2159, 3930, 2044, 22304, 3066]\n",
      "Original Text:  The Portage Township Schools Corp. ended it’s nearly 20-year school resource officer relationship with the Portage Police department Friday.\n",
      "In a written statement, schools Superintendent Amanda Alaniz said the Porter County Sheriff’s office will take over the two full-time resource officer positions at Portage High School and the city’s two middle schools. The statement also says Portage police can apply for any of a number of “supplemental” resource officer positions that will work in the schools.\n",
      "The move blindsided Portage Police Chief Troy Williams, a former resource officer at the high school, who said he heard about his officers’ dismissals from the Post-Tribune. For nearly 20 years, two Portage police officers, one at the high school and one roving between the two middle schools, have served as full-time resource officers for Portage schools, with the school district and the city splitting the salaries for those officers.\n",
      "“I’m shocked, even given the events that have transpired up to now,” Williams said. “Summarily dismissing an entire (police) department after nearly 20 years to the Portage Township Schools is a disgrace.\n",
      "“But, with the relationships we’ve built with students and staff, you’re going to dismiss us at that cost?”\n",
      "In keeping with a board approved overhaul of its safety program, which also offered extensive mental health services for students and families, the school district also will have a number of part-time resource officer positions, paying about $30 an hour, for officers from a number of jurisdictions.\n",
      "Alaniz was unavailable for additional comment late Friday, but, in her statement, Alaniz described a “unified partnership with the Porter County Sheriff’s office.”\n",
      "She also said students and families should “expect these (resource officers) to be trained in school safety techniques, know our students, protect them, and defend them, should the need arise.”\n",
      "The move Friday, which was followed by a letter from an Indianapolis-based law firm Church Church Hittle and Antrim indicating any memorandum of understanding between the schools and the Portage Police Department has expired.\n",
      "The law firm also suggested potential legal action if “the City of Portage does not comply with the expectations set forth in this letter.”\n",
      "The Friday announcement is the latest salvo in what has been a running dispute between the Portage Police department, Portage Mayor James Snyder and the school district.\n",
      "In June, the board approved its comprehensive safety plan, which removed Portage police officers from the full-time slots, opening all resource officer work to police from different jurisdictions.\n",
      "Several days later, the school district reversed its vote on the resource officer portion of the comprehensive plan, keeping two full-time Portage officers in the schools and opening the supplemental positions to officers from other districts.\n",
      "Prior to the comprehensive plan vote, just days after a school shooting in Noblesville, Ind., Snyder and Williams offered two additional Portage police officers to work at the Portage schools for no additional cost to the schools.\n",
      "The district rejected the officer, and the schools and city officials have gone back-and-forth through e-mails, meetings and even a press conference by Alaniz and Deb Porter, president of the Portage teachers union.\n",
      "For weeks, Alaniz has said there is no working MOU between the schools and the city.\n",
      "Snyder pounced on Alaniz and the board after learning of the removal of the Portage police officers Friday, indicating Alaniz was “learning from the Opriskos on how to collaborate, and that’s ‘you do what we say or else’.”\n",
      "Snyder was referring to Portage City Council President Mark Oprisko, D- At large, and Cheryl Oprisko, vice president of the Portage Township school board.\n",
      "Williams has filed to run for Oprisko’s seat on the school board. According to Porter County Elections listings Friday, he will oppose Joan Machuca.\n",
      "“Superintendent Alaniz has made this issue about herself and not the kids,” Snyder said, adding he and Williams have made numerous “concessions” to get an agreement on the resource officer dispute with the school system.\n",
      "“I’d be very apprehensive about sending my kids to (Portage Township schools), because this is all about one person, Amanda Alaniz,” Snyder said.\n",
      "School Board President Andy Maletta, who also is the economic development director for the City of Portage, recused himself from any votes on the schools’ new safety plan. He could not be reached for comment Friday.\n",
      "Responding to Snyder’s comments, Alaniz said her move was “in alignment” with what the board approved in June, so she did not get any additional approval from the board to dismiss the Portage police officers.\n",
      "“And, as for Mayor Snyder’s comments about me, I think they’re telling of his character, not mine.”\n",
      "Michael Gonzalez is a freelance reporter for the Post-Tribune. \n",
      "\n",
      "Tokenized Text:  ['the', 'portage', 'township', 'schools', 'corp', '.', 'ended', 'it', 'is', 'nearly', '20', '-', 'year', 'school', 'resource', 'officer', 'relationship', 'with', 'the', 'portage', 'police', 'department', 'friday', '.', 'in', 'a', 'written', 'statement', ',', 'schools', 'superintendent', 'amanda', 'alan', '##iz', 'said', 'the', 'porter', 'county', 'sheriff', 's', 'office', 'will', 'take', 'over', 'the', 'two', 'full', '-', 'time', 'resource', 'officer', 'positions', 'at', 'portage', 'high', 'school', 'and', 'the', 'city', 's', 'two', 'middle', 'schools', '.', 'the', 'statement', 'also', 'says', 'portage', 'police', 'can', 'apply', 'for', 'any', 'of', 'a', 'number', 'of', 'supplemental', 'resource', 'officer', 'positions', 'that', 'will', 'work', 'in', 'the', 'schools', '.', 'the', 'move', 'blinds', '##ided', 'portage', 'police', 'chief', 'troy', 'williams', ',', 'a', 'former', 'resource', 'officer', 'at', 'the', 'high', 'school', ',', 'who', 'said', 'he', 'heard', 'about', 'his', 'officers', 'dismissal', '##s', 'from', 'the', 'post', '-', 'tribune', '.', 'for', 'nearly', '20', 'years', ',', 'two', 'portage', 'police', 'officers', ',', 'one', 'at', 'the', 'high', 'school', 'and', 'one', 'ro', '##ving', 'between', 'the', 'two', 'middle', 'schools', ',', 'have', 'served', 'as', 'full', '-', 'time', 'resource', 'officers', 'for', 'portage', 'schools', ',', 'with', 'the', 'school', 'district', 'and', 'the', 'city', 'splitting', 'the', 'salaries', 'for', 'those', 'officers', '.', 'i', 'am', 'shocked', ',', 'even', 'given', 'the', 'events', 'that', 'have', 'trans', '##pired', 'up', 'to', 'now', ',', 'williams', 'said', '.', 'sum', '##mar', '##ily', 'dismissing', 'an', 'entire', '(', 'police', ')', 'department', 'after', 'nearly', '20', 'years', 'to', 'the', 'portage', 'township', 'schools', 'is', 'a', 'disgrace', '.', 'but', ',', 'with', 'the', 'relationships', 'we', 'have', 'built', 'with', 'students', 'and', 'staff', ',', 'you', 'are', 'going', 'to', 'dismiss', 'us', 'at', 'that', 'cost', '?', 'in', 'keeping', 'with', 'a', 'board', 'approved', 'overhaul', 'of', 'its', 'safety', 'program', ',', 'which', 'also', 'offered', 'extensive', 'mental', 'health', 'services', 'for', 'students', 'and', 'families', ',', 'the', 'school', 'district', 'also', 'will', 'have', 'a', 'number', 'of', 'part', '-', 'time', 'resource', 'officer', 'positions', ',', 'paying', 'about', '$', '30', 'an', 'hour', ',', 'for', 'officers', 'from', 'a', 'number', 'of', 'jurisdictions', '.', 'alan', '##iz', 'was', 'unavailable', 'for', 'additional', 'comment', 'late', 'friday', ',', 'but', ',', 'in', 'her', 'statement', ',', 'alan', '##iz', 'described', 'a', 'unified', 'partnership', 'with', 'the', 'porter', 'county', 'sheriff', 's', 'office', '.', 'she', 'also', 'said', 'students', 'and', 'families', 'should', 'expect', 'these', '(', 'resource', 'officers', ')', 'to', 'be', 'trained', 'in', 'school', 'safety', 'techniques', ',', 'know', 'our', 'students', ',', 'protect', 'them', ',', 'and', 'defend', 'them', ',', 'should', 'the', 'need', 'arise', '.', 'the', 'move', 'friday', ',', 'which', 'was', 'followed', 'by', 'a', 'letter', 'from', 'an', 'indianapolis', '-', 'based', 'law', 'firm', 'church', 'church', 'hit', '##tle', 'and', 'antrim', 'indicating', 'any', 'memorandum', 'of', 'understanding', 'between', 'the', 'schools', 'and', 'the', 'portage', 'police', 'department', 'has', 'expired', '.', 'the', 'law', 'firm', 'also', 'suggested', 'potential', 'legal', 'action', 'if', 'the', 'city', 'of', 'portage', 'does', 'not', 'comply', 'with', 'the', 'expectations', 'set', 'forth', 'in', 'this', 'letter', '.', 'the', 'friday', 'announcement', 'is', 'the', 'latest', 'sal', '##vo', 'in', 'what', 'has', 'been', 'a', 'running', 'dispute', 'between', 'the', 'portage', 'police', 'department', ',', 'portage', 'mayor', 'james', 'snyder', 'and', 'the', 'school', 'district', '.', 'in', 'june', ',', 'the', 'board', 'approved', 'its', 'comprehensive', 'safety', 'plan', ',', 'which', 'removed', 'portage', 'police', 'officers', 'from', 'the', 'full', '-', 'time', 'slots', ',', 'opening', 'all', 'resource', 'officer', 'work', 'to', 'police', 'from', 'different', 'jurisdictions', '.', 'several', 'days', 'later', ',', 'the', 'school', 'district', 'reversed', 'its', 'vote', 'on', 'the', 'resource', 'officer', 'portion', 'of', 'the', 'comprehensive', 'plan', ',', 'keeping', 'two', 'full', '-', 'time', 'portage', 'officers', 'in', 'the', 'schools', 'and', 'opening', 'the', 'supplemental', 'positions', 'to', 'officers', 'from', 'other', 'districts', '.', 'prior', 'to', 'the', 'comprehensive', 'plan', 'vote', ',', 'just', 'days', 'after', 'a', 'school', 'shooting', 'in', 'nobles', '##ville', ',', 'ind', '.', ',', 'snyder', 'and', 'williams', 'offered', 'two', 'additional', 'portage', 'police', 'officers', 'to', 'work', 'at', 'the', 'portage', 'schools', 'for', 'no', 'additional', 'cost', 'to', 'the', 'schools', '.', 'the', 'district', 'rejected', 'the', 'officer', ',', 'and', 'the', 'schools', 'and', 'city', 'officials', 'have', 'gone', 'back', '-', 'and', '-', 'forth', 'through', 'e', '-', 'mail', '##s', ',', 'meetings', 'and', 'even', 'a', 'press', 'conference', 'by', 'alan', '##iz', 'and', 'de', '##b', 'porter', ',', 'president', 'of', 'the', 'portage', 'teachers', 'union', '.', 'for', 'weeks', ',', 'alan', '##iz', 'has', 'said', 'there', 'is', 'no', 'working', 'mo', '##u', 'between', 'the', 'schools', 'and', 'the', 'city', '.', 'snyder', 'po', '##unced', 'on', 'alan', '##iz', 'and', 'the', 'board', 'after', 'learning', 'of', 'the', 'removal', 'of', 'the', 'portage', 'police', 'officers', 'friday', ',', 'indicating', 'alan', '##iz', 'was', 'learning', 'from', 'the', 'op', '##ris', '##kos', 'on', 'how', 'to', 'collaborate', ',', 'and', 'that', 'is', 'you', 'do', 'what', 'we', 'say', 'or', 'else', '.', 'snyder', 'was', 'referring', 'to', 'portage', 'city', 'council', 'president', 'mark', 'op', '##ris', '##ko', ',', 'd', '-', 'at', 'large', ',', 'and', 'cheryl', 'op', '##ris', '##ko', ',', 'vice', 'president', 'of', 'the', 'portage', 'township', 'school', 'board', '.', 'williams', 'has', 'filed', 'to', 'run', 'for', 'op', '##ris', '##ko', 's', 'seat', 'on', 'the', 'school', 'board', '.', 'according', 'to', 'porter', 'county', 'elections', 'listings', 'friday', ',', 'he', 'will', 'oppose', 'joan', 'mach', '##uca', '.', 'superintendent', 'alan', '##iz', 'has', 'made', 'this', 'issue', 'about', 'herself', 'and', 'not', 'the', 'kids', ',', 'snyder', 'said', ',', 'adding', 'he', 'and', 'williams', 'have', 'made', 'numerous', 'concessions', 'to', 'get', 'an', 'agreement', 'on', 'the', 'resource', 'officer', 'dispute', 'with', 'the', 'school', 'system', '.', 'i', 'would', 'be', 'very', 'app', '##re', '##hen', '##sive', 'about', 'sending', 'my', 'kids', 'to', '(', 'portage', 'township', 'schools', ')', ',', 'because', 'this', 'is', 'all', 'about', 'one', 'person', ',', 'amanda', 'alan', '##iz', ',', 'snyder', 'said', '.', 'school', 'board', 'president', 'andy', 'male', '##tta', ',', 'who', 'also', 'is', 'the', 'economic', 'development', 'director', 'for', 'the', 'city', 'of', 'portage', ',', 'rec', '##used', 'himself', 'from', 'any', 'votes', 'on', 'the', 'schools', 'new', 'safety', 'plan', '.', 'he', 'could', 'not', 'be', 'reached', 'for', 'comment', 'friday', '.', 'responding', 'to', 'snyder', 's', 'comments', ',', 'alan', '##iz', 'said', 'her', 'move', 'was', 'in', 'alignment', 'with', 'what', 'the', 'board', 'approved', 'in', 'june', ',', 'so', 'she', 'did', 'not', 'get', 'any', 'additional', 'approval', 'from', 'the', 'board', 'to', 'dismiss', 'the', 'portage', 'police', 'officers', '.', 'and', ',', 'as', 'for', 'mayor', 'snyder', 's', 'comments', 'about', 'me', ',', 'i', 'think', 'they', 'are', 'telling', 'of', 'his', 'character', ',', 'not', 'mine', '.', 'michael', 'gonzalez', 'is', 'a', 'freelance', 'reporter', 'for', 'the', 'post', '-', 'tribune', '.'] \n",
      "\n",
      "Token IDs:  [1996, 25140, 3545, 2816, 13058, 1012, 3092, 2009, 1521, 1055, 3053, 2322, 1011, 2095, 2082, 7692, 2961, 3276, 2007, 1996, 25140, 2610, 2533, 5958, 1012, 1999, 1037, 2517, 4861, 1010, 2816, 9133, 8282, 5070, 10993, 2056, 1996, 8716, 2221, 6458, 1521, 1055, 2436, 2097, 2202, 2058, 1996, 2048, 2440, 1011, 2051, 7692, 2961, 4460, 2012, 25140, 2152, 2082, 1998, 1996, 2103, 1521, 1055, 2048, 2690, 2816, 1012, 1996, 4861, 2036, 2758, 25140, 2610, 2064, 6611, 2005, 2151, 1997, 1037, 2193, 1997, 1523, 27024, 1524, 7692, 2961, 4460, 2008, 2097, 2147, 1999, 1996, 2816, 1012, 1996, 2693, 28279, 14097, 25140, 2610, 2708, 9553, 3766, 1010, 1037, 2280, 7692, 2961, 2012, 1996, 2152, 2082, 1010, 2040, 2056, 2002, 2657, 2055, 2010, 3738, 1521, 15322, 2015, 2013, 1996, 2695, 1011, 10969, 1012, 2005, 3053, 2322, 2086, 1010, 2048, 25140, 2610, 3738, 1010, 2028, 2012, 1996, 2152, 2082, 1998, 2028, 20996, 6455, 2090, 1996, 2048, 2690, 2816, 1010, 2031, 2366, 2004, 2440, 1011, 2051, 7692, 3738, 2005, 25140, 2816, 1010, 2007, 1996, 2082, 2212, 1998, 1996, 2103, 14541, 1996, 20566, 2005, 2216, 3738, 1012, 1523, 1045, 1521, 1049, 7135, 1010, 2130, 2445, 1996, 2824, 2008, 2031, 9099, 21649, 2039, 2000, 2085, 1010, 1524, 3766, 2056, 1012, 1523, 7680, 7849, 6588, 28913, 2019, 2972, 1006, 2610, 1007, 2533, 2044, 3053, 2322, 2086, 2000, 1996, 25140, 3545, 2816, 2003, 1037, 29591, 1012, 1523, 2021, 1010, 2007, 1996, 6550, 2057, 1521, 2310, 2328, 2007, 2493, 1998, 3095, 1010, 2017, 1521, 2128, 2183, 2000, 19776, 2149, 2012, 2008, 3465, 1029, 1524, 1999, 4363, 2007, 1037, 2604, 4844, 18181, 1997, 2049, 3808, 2565, 1010, 2029, 2036, 3253, 4866, 5177, 2740, 2578, 2005, 2493, 1998, 2945, 1010, 1996, 2082, 2212, 2036, 2097, 2031, 1037, 2193, 1997, 2112, 1011, 2051, 7692, 2961, 4460, 1010, 7079, 2055, 1002, 2382, 2019, 3178, 1010, 2005, 3738, 2013, 1037, 2193, 1997, 17370, 1012, 5070, 10993, 2001, 20165, 2005, 3176, 7615, 2397, 5958, 1010, 2021, 1010, 1999, 2014, 4861, 1010, 5070, 10993, 2649, 1037, 1523, 10562, 5386, 2007, 1996, 8716, 2221, 6458, 1521, 1055, 2436, 1012, 1524, 2016, 2036, 2056, 2493, 1998, 2945, 2323, 1523, 5987, 2122, 1006, 7692, 3738, 1007, 2000, 2022, 4738, 1999, 2082, 3808, 5461, 1010, 2113, 2256, 2493, 1010, 4047, 2068, 1010, 1998, 6985, 2068, 1010, 2323, 1996, 2342, 13368, 1012, 1524, 1996, 2693, 5958, 1010, 2029, 2001, 2628, 2011, 1037, 3661, 2013, 2019, 9506, 1011, 2241, 2375, 3813, 2277, 2277, 2718, 9286, 1998, 24142, 8131, 2151, 20336, 1997, 4824, 2090, 1996, 2816, 1998, 1996, 25140, 2610, 2533, 2038, 13735, 1012, 1996, 2375, 3813, 2036, 4081, 4022, 3423, 2895, 2065, 1523, 1996, 2103, 1997, 25140, 2515, 2025, 14037, 2007, 1996, 10908, 2275, 5743, 1999, 2023, 3661, 1012, 1524, 1996, 5958, 8874, 2003, 1996, 6745, 16183, 6767, 1999, 2054, 2038, 2042, 1037, 2770, 7593, 2090, 1996, 25140, 2610, 2533, 1010, 25140, 3664, 2508, 17840, 1998, 1996, 2082, 2212, 1012, 1999, 2238, 1010, 1996, 2604, 4844, 2049, 7721, 3808, 2933, 1010, 2029, 3718, 25140, 2610, 3738, 2013, 1996, 2440, 1011, 2051, 19832, 1010, 3098, 2035, 7692, 2961, 2147, 2000, 2610, 2013, 2367, 17370, 1012, 2195, 2420, 2101, 1010, 1996, 2082, 2212, 11674, 2049, 3789, 2006, 1996, 7692, 2961, 4664, 1997, 1996, 7721, 2933, 1010, 4363, 2048, 2440, 1011, 2051, 25140, 3738, 1999, 1996, 2816, 1998, 3098, 1996, 27024, 4460, 2000, 3738, 2013, 2060, 4733, 1012, 3188, 2000, 1996, 7721, 2933, 3789, 1010, 2074, 2420, 2044, 1037, 2082, 5008, 1999, 13969, 3077, 1010, 27427, 1012, 1010, 17840, 1998, 3766, 3253, 2048, 3176, 25140, 2610, 3738, 2000, 2147, 2012, 1996, 25140, 2816, 2005, 2053, 3176, 3465, 2000, 1996, 2816, 1012, 1996, 2212, 5837, 1996, 2961, 1010, 1998, 1996, 2816, 1998, 2103, 4584, 2031, 2908, 2067, 1011, 1998, 1011, 5743, 2083, 1041, 1011, 5653, 2015, 1010, 6295, 1998, 2130, 1037, 2811, 3034, 2011, 5070, 10993, 1998, 2139, 2497, 8716, 1010, 2343, 1997, 1996, 25140, 5089, 2586, 1012, 2005, 3134, 1010, 5070, 10993, 2038, 2056, 2045, 2003, 2053, 2551, 9587, 2226, 2090, 1996, 2816, 1998, 1996, 2103, 1012, 17840, 13433, 22392, 2006, 5070, 10993, 1998, 1996, 2604, 2044, 4083, 1997, 1996, 8208, 1997, 1996, 25140, 2610, 3738, 5958, 1010, 8131, 5070, 10993, 2001, 1523, 4083, 2013, 1996, 6728, 6935, 15710, 2006, 2129, 2000, 20880, 1010, 1998, 2008, 1521, 1055, 1520, 2017, 2079, 2054, 2057, 2360, 2030, 2842, 1521, 1012, 1524, 17840, 2001, 7727, 2000, 25140, 2103, 2473, 2343, 2928, 6728, 6935, 3683, 1010, 1040, 1011, 2012, 2312, 1010, 1998, 19431, 6728, 6935, 3683, 1010, 3580, 2343, 1997, 1996, 25140, 3545, 2082, 2604, 1012, 3766, 2038, 6406, 2000, 2448, 2005, 6728, 6935, 3683, 1521, 1055, 2835, 2006, 1996, 2082, 2604, 1012, 2429, 2000, 8716, 2221, 3864, 26213, 5958, 1010, 2002, 2097, 15391, 7437, 24532, 18100, 1012, 1523, 9133, 5070, 10993, 2038, 2081, 2023, 3277, 2055, 2841, 1998, 2025, 1996, 4268, 1010, 1524, 17840, 2056, 1010, 5815, 2002, 1998, 3766, 2031, 2081, 3365, 1523, 20638, 1524, 2000, 2131, 2019, 3820, 2006, 1996, 7692, 2961, 7593, 2007, 1996, 2082, 2291, 1012, 1523, 1045, 1521, 1040, 2022, 2200, 10439, 2890, 10222, 12742, 2055, 6016, 2026, 4268, 2000, 1006, 25140, 3545, 2816, 1007, 1010, 2138, 2023, 2003, 2035, 2055, 2028, 2711, 1010, 8282, 5070, 10993, 1010, 1524, 17840, 2056, 1012, 2082, 2604, 2343, 5557, 3287, 5946, 1010, 2040, 2036, 2003, 1996, 3171, 2458, 2472, 2005, 1996, 2103, 1997, 25140, 1010, 28667, 13901, 2370, 2013, 2151, 4494, 2006, 1996, 2816, 1521, 2047, 3808, 2933, 1012, 2002, 2071, 2025, 2022, 2584, 2005, 7615, 5958, 1012, 14120, 2000, 17840, 1521, 1055, 7928, 1010, 5070, 10993, 2056, 2014, 2693, 2001, 1523, 1999, 12139, 1524, 2007, 2054, 1996, 2604, 4844, 1999, 2238, 1010, 2061, 2016, 2106, 2025, 2131, 2151, 3176, 6226, 2013, 1996, 2604, 2000, 19776, 1996, 25140, 2610, 3738, 1012, 1523, 1998, 1010, 2004, 2005, 3664, 17840, 1521, 1055, 7928, 2055, 2033, 1010, 1045, 2228, 2027, 1521, 2128, 4129, 1997, 2010, 2839, 1010, 2025, 3067, 1012, 1524, 2745, 10121, 2003, 1037, 15919, 6398, 2005, 1996, 2695, 1011, 10969, 1012]\n",
      "Original Text:  Global investors including EQT Partners and KKR & Co. are looking at bidding for the Long Beach Container Terminal in Southern California, as are seasoned ship operators Seaspan Corp. and Hyundai Merchant Marine, according to people directly involved in the sale.\n",
      "The sale of the major gateway for U.S. seaborne trade with China is expected to fetch as much as $2 billion and comes as its owner—China’s Cosco Shipping Holdings Co.—looks to dispose of the terminal by the first half of next year.\n",
      "... \n",
      "\n",
      "Tokenized Text:  ['global', 'investors', 'including', 'e', '##q', '##t', 'partners', 'and', 'k', '##kr', '&', 'co', '.', 'are', 'looking', 'at', 'bidding', 'for', 'the', 'long', 'beach', 'container', 'terminal', 'in', 'southern', 'california', ',', 'as', 'are', 'seasoned', 'ship', 'operators', 'seas', '##pan', 'corp', '.', 'and', 'hyundai', 'merchant', 'marine', ',', 'according', 'to', 'people', 'directly', 'involved', 'in', 'the', 'sale', '.', 'the', 'sale', 'of', 'the', 'major', 'gateway', 'for', 'you', '.', 's', '.', 'sea', '##borne', 'trade', 'with', 'china', 'is', 'expected', 'to', 'fetch', 'as', 'much', 'as', '$', '2', 'billion', 'and', 'comes', 'as', 'its', 'owner', 'china', 's', 'co', '##sco', 'shipping', 'holdings', 'co', '.', 'looks', 'to', 'dispose', 'of', 'the', 'terminal', 'by', 'the', 'first', 'half', 'of', 'next', 'year', '.', '.', '.', '.'] \n",
      "\n",
      "Token IDs:  [3795, 9387, 2164, 1041, 4160, 2102, 5826, 1998, 1047, 21638, 1004, 2522, 1012, 2024, 2559, 2012, 17534, 2005, 1996, 2146, 3509, 11661, 5536, 1999, 2670, 2662, 1010, 2004, 2024, 28223, 2911, 9224, 11915, 9739, 13058, 1012, 1998, 25983, 6432, 3884, 1010, 2429, 2000, 2111, 3495, 2920, 1999, 1996, 5096, 1012, 1996, 5096, 1997, 1996, 2350, 11909, 2005, 1057, 1012, 1055, 1012, 2712, 19288, 3119, 2007, 2859, 2003, 3517, 2000, 18584, 2004, 2172, 2004, 1002, 1016, 4551, 1998, 3310, 2004, 2049, 3954, 1517, 2859, 1521, 1055, 2522, 9363, 7829, 9583, 2522, 1012, 1517, 3504, 2000, 27764, 1997, 1996, 5536, 2011, 1996, 2034, 2431, 1997, 2279, 2095, 1012, 1012, 1012, 1012]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#lemmatization and removing stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    def is_english_word(word):\n",
    "        \"\"\"Function to filter out non-English words.\"\"\"\n",
    "        return bool(re.match(r'^[a-zA-Z]+$', word))\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    # Remove non-English characters and punctuation\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Removes non-ASCII characters\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print('Original Text: ', df['train']['headline'][i], '\\n')\n",
    "    print('Tokenized Text: ', tokenizer.tokenize(preprocess(df['train']['headline'][i])), '\\n')\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['headline'][i])))\n",
    "\n",
    "for i in range(2):\n",
    "    print('Original Text: ', df['train']['body'][i], '\\n')\n",
    "    print('Tokenized Text: ', tokenizer.tokenize(preprocess(df['train']['body'][i])), '\\n')\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['body'][i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ERySJZcQBp9_"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    text = examples[\"body\"]\n",
    "    labels = examples[\"political_leaning\"]  \n",
    "    \n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors = \"np\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 512\n",
    "        )\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = [label2id[label] for label in labels]  \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "38\n",
      "0\n",
      "15\n",
      "16266\n",
      "89794\n"
     ]
    }
   ],
   "source": [
    "texts = df['train']['headline']\n",
    "\n",
    "# Handle None or missing values by filtering out None entries\n",
    "text_lengths = [len(text.split(' ')) if text is not None else 0 for text in texts]\n",
    "\n",
    "print(min(text_lengths))\n",
    "print(max(text_lengths))\n",
    "\n",
    "# Count how many texts have 300 or more words\n",
    "print(sum([1 for length in text_lengths if length >= 300]))\n",
    "\n",
    "# Repeat for the 'body' column\n",
    "texts = df['train']['body']\n",
    "\n",
    "# Handle None or missing values by filtering out None entries\n",
    "text_lengths = [len(text.split()) if text is not None else 0 for text in texts]\n",
    "\n",
    "print(min(text_lengths))\n",
    "print(max(text_lengths))\n",
    "\n",
    "# Count how many texts have 300 or more words\n",
    "print(sum([1 for length in text_lengths if length >= 300]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "YmGXgdNQCb7e"
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "  model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ZgXdBTYUDLz6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 136735/136735 [01:55<00:00, 1179.22 examples/s]\n",
      "Map: 100%|██████████| 15193/15193 [00:14<00:00, 1015.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 136735\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15193\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = df.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "IjSOWH6wDNHw"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "KJvaBgtkD963"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "  predictions, labels = p\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return {\"accuracy\": accuracy.compute(predictions=predictions\n",
    "                                       , references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8xiaVnUaF1Yf"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type='SEQ_CLS',\n",
    "                         r = 4,\n",
    "                         lora_alpha=32,\n",
    "                         lora_dropout=0.01,\n",
    "                         target_modules = ['q_lin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o4hduUwTGnN5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 630,532 || all params: 67,587,080 || trainable%: 0.9329\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=4, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.distilbert.embeddings.word_embeddings.weight: requires_grad=False\n",
      "base_model.model.distilbert.embeddings.position_embeddings.weight: requires_grad=False\n",
      "base_model.model.distilbert.embeddings.LayerNorm.weight: requires_grad=False\n",
      "base_model.model.distilbert.embeddings.LayerNorm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.base_layer.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.base_layer.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight: requires_grad=False\n",
      "base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias: requires_grad=False\n",
      "base_model.model.pre_classifier.original_module.weight: requires_grad=False\n",
      "base_model.model.pre_classifier.original_module.bias: requires_grad=False\n",
      "base_model.model.pre_classifier.modules_to_save.default.weight: requires_grad=True\n",
      "base_model.model.pre_classifier.modules_to_save.default.bias: requires_grad=True\n",
      "base_model.model.classifier.original_module.weight: requires_grad=False\n",
      "base_model.model.classifier.original_module.bias: requires_grad=False\n",
      "base_model.model.classifier.modules_to_save.default.weight: requires_grad=True\n",
      "base_model.model.classifier.modules_to_save.default.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ORBVjXnGx19"
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"\"+model_checkpoint+\"lora-txt\",\n",
    "    learning_rate = lr,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    num_train_epochs = num_epochs,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "hmS4TS65IDDV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/r66735gn7xx2tdk0357s_k_m0000gn/T/ipykernel_86100/1961861285.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    eval_dataset = tokenized_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfzk-YnMJL0V"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='235' max='66025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  235/66025 01:50 < 8:39:20, 2.11 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jn1iHMyJNtM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALEJANDRO\\AppData\\Local\\Temp\\ipykernel_11648\\1630884471.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"trained_model_gral_imbd.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions\n",
      "RIGHT\n",
      "RIGHT\n",
      "RIGHT\n",
      "RIGHT\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dict = torch.load(\"trained_model_gral_imbd.pth\", map_location=device)\n",
    "\n",
    "text_list = ['''President-elect Trump announced on Tuesday night that he intends to appoint Linda McMahon, former CEO of World Wrestling Entertainment (WWE), to lead the Department of Education. His announcement, which was posted on Truth Social, came hours after two sources told Fox News that McMahon was likely to be picked. \"It is my great honor to announce that Linda McMahon, former Administrator of the Small Business Administration, will be the United States Secretary of Education,\" Trump's statement read.\n",
    "\"As Secretary of Education, Linda will fight tirelessly to expand Choice to every State in America, and empower parents to make the best Education decisions for their families,\" the press release added. \"Linda served for two years on the Connecticut Board of Education, where she was one of fifteen members overseeing all Public Education in the State, including its Technical High School system.\"''', \n",
    "             '''Donald Trump believes presidents have almost absolute power. In his second term, there will be few political or legal restraints to check him. The president-elects sweeping victory over Vice President Kamala Harris suddenly turned the theoretical notion that he will indulge his autocratic instincts into a genuine possibility.When Trump returns to the White House in January as one of the most powerful presidents in history, hell be able to take advantage of his own filleting of guardrails during his first presidency, which he continued through legal maneuverings out of office.''',\n",
    "             '''Nearly 100 Democrats, including Salud Carbajal, requested the Ethics Committee release its report on former Congressman Matt Gaetz's misconduct allegations. The letter, led by Rep. Sean Casten, emphasized that the Senate needs information for Gaetz's attorney general nomination. House Speaker Mike Johnson opposed releasing the report, stating Gaetz is now a \"private citizen\" and outside the panel's jurisdiction.'''\n",
    "             , ''' A South Dakota judge dismissed a lawsuit from the anti-abortion group Life Defense targeting an abortion rights measure that voters later rejected.\n",
    "Judge John Pekas dismissed the lawsuit at the request of Life Defense, which had challenged the ballot measure's petitions.\n",
    "Voters in nine states, including South Dakota, rejected abortion rights measures during the November election. '''\n",
    "             ]\n",
    "model.to('cuda')\n",
    "print('Trained model predictions')\n",
    "for text in text_list:\n",
    "  inputs = tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
    "\n",
    "  logits = model(inputs).logits\n",
    "  predictions = torch.max(logits,1).indices\n",
    "\n",
    "  #print(f'{text} - {id2label[predictions.tolist()[0]]}')\n",
    "  print(f'{id2label[predictions.tolist()[0]]}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_lda_implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
