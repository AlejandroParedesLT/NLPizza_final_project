{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **word2vec implementation for political leanings**\n",
    "\n",
    "https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/\n",
    "\n",
    "https://www.kaggle.com/datasets/umbertogriffo/googles-trained-word2vec-model-in-python/code?datasetId=12162&sortBy=voteCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.matutils import corpus2csc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50269, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_publish</th>\n",
       "      <th>outlet</th>\n",
       "      <th>headline</th>\n",
       "      <th>lead</th>\n",
       "      <th>body</th>\n",
       "      <th>authors</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>political_leaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52972702</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Iran Says It Exceeded Enriched Uranium Cap</td>\n",
       "      <td>Iran's foreign minister confirms that his coun...</td>\n",
       "      <td>Iran Says It Exceeded Enriched Uranium Cap\\nIr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.npr.org</td>\n",
       "      <td>https://www.npr.org/2019/07/01/737600999/iran-...</td>\n",
       "      <td>LEFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4037078</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Bombing Kills Dozens in Kabul as Taliban Talks...</td>\n",
       "      <td>Attackers set off bombs that wrecked a war mus...</td>\n",
       "      <td>KABUL, Afghanistan — A complex attack includin...</td>\n",
       "      <td>Thomas Gibbons-Neff;Rod Nordland</td>\n",
       "      <td>www.nytimes.com</td>\n",
       "      <td>https://www.nytimes.com/2019/07/01/world/asia/...</td>\n",
       "      <td>LEFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52904870</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>NPR</td>\n",
       "      <td>How Long Will The Current U.S. Economic Expans...</td>\n",
       "      <td>NPR's Steve Inskeep talks to David Wessel, dir...</td>\n",
       "      <td>How Long Will The Current U.S. Economic Expans...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.npr.org</td>\n",
       "      <td>https://www.npr.org/2019/07/01/737535414/how-l...</td>\n",
       "      <td>LEFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18311035</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>BBC</td>\n",
       "      <td>Black girls 'perceived as less innocent by US ...</td>\n",
       "      <td>New research from The Georgetown Law Center on...</td>\n",
       "      <td>Video\\nNew research from The Georgetown Law Ce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.bbc.com</td>\n",
       "      <td>http://www.bbc.com/news/av/world-us-canada-404...</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4061861</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Pete Buttigieg Raised $24.8 Million in Second ...</td>\n",
       "      <td>Mr. Buttigieg’s total is the latest evidence t...</td>\n",
       "      <td>WASHINGTON — Mayor Pete Buttigieg of South Ben...</td>\n",
       "      <td>Thomas Kaplan</td>\n",
       "      <td>www.nytimes.com</td>\n",
       "      <td>https://www.nytimes.com/2019/07/01/us/politics...</td>\n",
       "      <td>LEFT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id         date_publish              outlet  \\\n",
       "0  52972702  2019-07-01 00:00:00                 NPR   \n",
       "1   4037078  2019-07-01 00:00:00  The New York Times   \n",
       "2  52904870  2019-07-01 00:00:00                 NPR   \n",
       "3  18311035  2019-07-01 00:00:00                 BBC   \n",
       "4   4061861  2019-07-01 00:00:00  The New York Times   \n",
       "\n",
       "                                            headline  \\\n",
       "0         Iran Says It Exceeded Enriched Uranium Cap   \n",
       "1  Bombing Kills Dozens in Kabul as Taliban Talks...   \n",
       "2  How Long Will The Current U.S. Economic Expans...   \n",
       "3  Black girls 'perceived as less innocent by US ...   \n",
       "4  Pete Buttigieg Raised $24.8 Million in Second ...   \n",
       "\n",
       "                                                lead  \\\n",
       "0  Iran's foreign minister confirms that his coun...   \n",
       "1  Attackers set off bombs that wrecked a war mus...   \n",
       "2  NPR's Steve Inskeep talks to David Wessel, dir...   \n",
       "3  New research from The Georgetown Law Center on...   \n",
       "4  Mr. Buttigieg’s total is the latest evidence t...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Iran Says It Exceeded Enriched Uranium Cap\\nIr...   \n",
       "1  KABUL, Afghanistan — A complex attack includin...   \n",
       "2  How Long Will The Current U.S. Economic Expans...   \n",
       "3  Video\\nNew research from The Georgetown Law Ce...   \n",
       "4  WASHINGTON — Mayor Pete Buttigieg of South Ben...   \n",
       "\n",
       "                            authors           domain  \\\n",
       "0                               NaN      www.npr.org   \n",
       "1  Thomas Gibbons-Neff;Rod Nordland  www.nytimes.com   \n",
       "2                               NaN      www.npr.org   \n",
       "3                               NaN      www.bbc.com   \n",
       "4                     Thomas Kaplan  www.nytimes.com   \n",
       "\n",
       "                                                 url political_leaning  \n",
       "0  https://www.npr.org/2019/07/01/737600999/iran-...              LEFT  \n",
       "1  https://www.nytimes.com/2019/07/01/world/asia/...              LEFT  \n",
       "2  https://www.npr.org/2019/07/01/737535414/how-l...              LEFT  \n",
       "3  http://www.bbc.com/news/av/world-us-canada-404...         UNDEFINED  \n",
       "4  https://www.nytimes.com/2019/07/01/us/politics...              LEFT  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"./data/2019_2.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#lemmatization and removing stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    def is_english_word(word):\n",
    "        \"\"\"Function to filter out non-English words.\"\"\"\n",
    "        return bool(re.match(r'^[a-zA-Z]+$', word))\n",
    "    text = text.lower()\n",
    "    editorials_to_exclude = [\"cnn\", \"fox\", \"reuters\"]\n",
    "    for editorial in editorials_to_exclude:\n",
    "        text = re.sub(r\"\\b\" + re.escape(editorial) + r\"\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = contractions.fix(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [\n",
    "            lemmatizer.lemmatize(word) \n",
    "            for word in words \n",
    "            if word not in stop_words and is_english_word(word)\n",
    "        ]\n",
    "    words = [re.sub(r'[^\\w\\s]', '', token) for token in words if re.sub(r'[^\\w\\s]', '', token)]\n",
    "    words = [word for word in words if word]\n",
    "    return words[:5000]\n",
    "\n",
    "# Create Word2Vec Corpus for headline and body\n",
    "class MyCorpus:\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __iter__(self):\n",
    "        for text in self.texts:\n",
    "            yield preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = df['headline']#.apply(preprocess)\n",
    "body = df['body']#.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word2Vec model for headline and body separately\n",
    "headline_corpus = MyCorpus(headline)\n",
    "body_corpus = MyCorpus(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_model = Word2Vec(sentences=headline_corpus) #,vector_size=100, window=5, min_count=1, workers=4)\n",
    "body_model = Word2Vec(sentences=body_corpus) #,vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/7511 is trump\n",
      "word #1/7511 is say\n",
      "word #2/7511 is new\n",
      "word #3/7511 is police\n",
      "word #4/7511 is u\n",
      "word #5/7511 is china\n",
      "word #6/7511 is democrat\n",
      "word #7/7511 is shooting\n",
      "word #8/7511 is house\n",
      "word #9/7511 is johnson\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(headline_model.wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(headline_model.wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.23672897  0.5591203   1.0645581   0.73366064 -1.3328309  -0.7641441\n",
      "  1.0223461   1.0448697  -1.1596106  -1.459425   -0.02159295 -0.63329655\n",
      " -0.29220298  0.39366356 -0.06688254 -0.66103786 -0.18051563 -0.8111985\n",
      " -0.9686123  -1.0873249   0.56819475 -0.10098366  0.3862931  -0.11523768\n",
      "  0.91793865 -0.27058858 -0.66611165 -0.7871672  -0.42913568 -0.02408917\n",
      "  0.5271      0.08537828  0.97727746 -0.40852332 -0.31020248 -0.4316396\n",
      "  0.08021104 -0.1384542   0.6650847  -1.6350652  -0.65189356  0.4250843\n",
      " -0.33790618 -1.0885394   0.40004852 -0.9645746  -0.6471147   0.9958767\n",
      "  0.28874108  0.90407944 -0.23221192  0.28743586 -0.8142548  -0.12371537\n",
      "  1.4398979   0.04213689  0.62624335 -0.29920742 -0.06238396 -0.56488794\n",
      " -0.08009458  0.2849092   0.8168788  -0.37240824 -1.0314384   0.02255967\n",
      "  0.11279092  0.15795445 -1.716002    0.33594173  0.04046352  0.5214024\n",
      " -0.84902894 -0.19226687  0.8210675   0.46776435  0.36002657  0.59457755\n",
      " -1.177098   -0.31923318  0.38619328 -0.3584497   0.2776511   0.8007132\n",
      " -0.07258649  0.12303361  0.47656775 -0.73783475 -0.8464869   0.39354345\n",
      " -0.30562475 -0.04564017 -0.17559984  0.597943    1.224161    0.67286325\n",
      " -1.1374575  -0.5164369   0.36446968  0.9394037 ]\n",
      "[-0.23672897  0.5591203   1.0645581   0.73366064 -1.3328309  -0.7641441\n",
      "  1.0223461   1.0448697  -1.1596106  -1.459425   -0.02159295 -0.63329655\n",
      " -0.29220298  0.39366356 -0.06688254 -0.66103786 -0.18051563 -0.8111985\n",
      " -0.9686123  -1.0873249   0.56819475 -0.10098366  0.3862931  -0.11523768\n",
      "  0.91793865 -0.27058858 -0.66611165 -0.7871672  -0.42913568 -0.02408917\n",
      "  0.5271      0.08537828  0.97727746 -0.40852332 -0.31020248 -0.4316396\n",
      "  0.08021104 -0.1384542   0.6650847  -1.6350652  -0.65189356  0.4250843\n",
      " -0.33790618 -1.0885394   0.40004852 -0.9645746  -0.6471147   0.9958767\n",
      "  0.28874108  0.90407944 -0.23221192  0.28743586 -0.8142548  -0.12371537\n",
      "  1.4398979   0.04213689  0.62624335 -0.29920742 -0.06238396 -0.56488794\n",
      " -0.08009458  0.2849092   0.8168788  -0.37240824 -1.0314384   0.02255967\n",
      "  0.11279092  0.15795445 -1.716002    0.33594173  0.04046352  0.5214024\n",
      " -0.84902894 -0.19226687  0.8210675   0.46776435  0.36002657  0.59457755\n",
      " -1.177098   -0.31923318  0.38619328 -0.3584497   0.2776511   0.8007132\n",
      " -0.07258649  0.12303361  0.47656775 -0.73783475 -0.8464869   0.39354345\n",
      " -0.30562475 -0.04564017 -0.17559984  0.597943    1.224161    0.67286325\n",
      " -1.1374575  -0.5164369   0.36446968  0.9394037 ]\n"
     ]
    }
   ],
   "source": [
    "vector_Trump = headline_model.wv['trump']\n",
    "print(vector_Trump)\n",
    "\n",
    "vector_Abortion = headline_model.wv['abortion']\n",
    "print(vector_Trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "headline_model.save('./models/headline_model')\n",
    "body_model.save('./models/body_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7511 - Embedding Dim: 100\n",
      "[('pelosi', 0.795551598072052), ('lawmaker', 0.7479333281517029), ('dems', 0.746344268321991)]\n",
      "[('walsh', 0.9306271076202393), ('arpaio', 0.9298386573791504), ('kamala', 0.8947133421897888)]\n",
      "[('temporarily', 0.9445401430130005), ('allows', 0.9336287379264832), ('enforcement', 0.9268746376037598)]\n",
      "[('walsh', 0.9102807641029358), ('arpaio', 0.8938730359077454), ('scarborough', 0.8577986359596252)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "\n",
    "# Retrieve the weights from the model. This is used for initializing the weights\n",
    "# in a Keras Embedding layer later\n",
    "w2v_weights = headline_model.wv.vectors\n",
    "vocab_size, embedding_size = w2v_weights.shape\n",
    "\n",
    "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))\n",
    "\n",
    "# Some validation on the quality of the Word2Vec model\n",
    "print(headline_model.wv.most_similar('trump', topn=3))\n",
    "print(headline_model.wv.most_similar('biden', topn=3))\n",
    "print(headline_model.wv.most_similar('abortion', topn=3))\n",
    "print(headline_model.wv.most_similar(positive=['trump', 'biden'], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2token(word):\n",
    "    try:\n",
    "        return headline_model.wv.vocab[word].index\n",
    "    except KeyError:\n",
    "        return 0\n",
    "def token2word(token):\n",
    "    return headline_model.wv.index_to_key[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Sample random words from model dictionary\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m random_i \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[43mvocab_size\u001b[49m), n_samples)\n\u001b[0;32m      5\u001b[0m random_w \u001b[38;5;241m=\u001b[39m [token2word(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m random_i]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Generate Word2Vec embeddings of each word\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "n_samples = 500\n",
    "# Sample random words from model dictionary\n",
    "random_i = random.sample(range(vocab_size), n_samples)\n",
    "random_w = [token2word(i) for i in random_i]\n",
    "\n",
    "# Generate Word2Vec embeddings of each word\n",
    "word_vecs = np.array([headline_model[w] for w in random_w])\n",
    "\n",
    "# Apply t-SNE to Word2Vec embeddings, reducing to 2 dims\n",
    "tsne = TSNE()\n",
    "tsne_e = tsne.fit_transform(word_vecs)\n",
    "\n",
    "# Plot t-SNE result\n",
    "plt.figure(figsize=(32, 32))\n",
    "plt.scatter(tsne_e[:, 0], tsne_e[:, 1], marker='o', c=range(len(random_w)), cmap=plt.get_cmap('Spectral'))\n",
    "\n",
    "for label, x, y, in zip(random_w, tsne_e[:, 0], tsne_e[:, 1]):\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y), xytext=(0, 15),\n",
    "                 textcoords='offset points', ha='right', va='bottom',\n",
    "                 bbox=dict(boxstyle='round, pad=0.2', fc='yellow', alpha=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/49643974/how-to-do-text-classification-using-word2vec\n",
    "\n",
    "https://stackoverflow.com/questions/57525190/text-classification-with-word2vec\n",
    "\n",
    "https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.Similarity\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "\n",
    "https://www.kaggle.com/code/guichristmann/lstm-classification-model-with-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Convert text to sequences of Word2Vec embeddings\n",
    "def get_word2vec_sequence(text, model):\n",
    "    words = preprocess(text)\n",
    "    word_vectors = [torch.tensor(model.wv[word], dtype=torch.float32) for word in words if word in model.wv]\n",
    "    return torch.stack(word_vectors) if word_vectors else torch.zeros((1, model.vector_size))\n",
    "\n",
    "# Convert the corpus into padded sequences\n",
    "def get_padded_sequences(corpus, model, max_length=500):\n",
    "    sequences = [get_word2vec_sequence(text, model) for text in corpus]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)  # Pad to the longest sequence\n",
    "    return padded_sequences[:, :max_length]  # Optional: Truncate to `max_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliticalLeaningDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences\n",
    "headline_sequences = get_padded_sequences(headline_corpus, headline_model)\n",
    "body_sequences = get_padded_sequences(body_corpus, body_model)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "political_leaning_encoded = label_encoder.fit_transform(df['political_leaning'])\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    headline_sequences, political_leaning_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = PoliticalLeaningDataset(X_train, y_train)\n",
    "test_dataset = PoliticalLeaningDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader instances for batching\n",
    "train_dataset = PoliticalLeaningDataset(X_train, y_train)\n",
    "test_dataset = PoliticalLeaningDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class PoliticalLeaningModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PoliticalLeaningModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, output_dim)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_dim = embeddings.shape[1]  # This is the size of the feature vector (headline + body)\n",
    "output_dim = len(label_encoder.classes_)  # Number of political leaning classes\n",
    "model = PoliticalLeaningModel(input_dim, output_dim)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict political leaning on new data\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor(data, dtype=torch.float32))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.numpy()\n",
    "\n",
    "# Predict political leaning for test data\n",
    "predicted_labels = predict(model, X_test)\n",
    "\n",
    "# Decode the predicted labels back to original political leaning classes\n",
    "predicted_political_leaning = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Print first 10 predictions\n",
    "for i in range(10):\n",
    "    print(f\"Predicted: {predicted_political_leaning[i]}, Actual: {label_encoder.inverse_transform([y_test[i]])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre trained word2vec**\n",
    "\n",
    "https://radimrehurek.com/gensim/similarities/docsim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### document2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    TaggedDocument(\"I love machine learning\", [0]),\n",
    "    TaggedDocument(\"Machine learning is the future\", [1]),\n",
    "    TaggedDocument(\"Deep learning is a subset of machine learning\", [2])\n",
    "]\n",
    "\n",
    "# Train Doc2Vec model\n",
    "model = Doc2Vec(documents, vector_size=100, window=5, min_count=2, epochs=40)\n",
    "\n",
    "# Infer vector for a new document\n",
    "new_document = \"I enjoy natural language processing\"\n",
    "new_vector = model.infer_vector(new_document.split())\n",
    "\n",
    "# Find most similar documents\n",
    "similar_docs = model.dv.most_similar([new_vector])\n",
    "\n",
    "print(similar_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_lda_implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
