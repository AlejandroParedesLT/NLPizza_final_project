{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **word2vec implementation for political leanings**\n",
    "\n",
    "https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/\n",
    "\n",
    "https://www.kaggle.com/datasets/umbertogriffo/googles-trained-word2vec-model-in-python/code?datasetId=12162&sortBy=voteCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.matutils import corpus2csc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50269, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_publish</th>\n",
       "      <th>outlet</th>\n",
       "      <th>headline</th>\n",
       "      <th>lead</th>\n",
       "      <th>body</th>\n",
       "      <th>authors</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>political_leaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52972702</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Iran Says It Exceeded Enriched Uranium Cap</td>\n",
       "      <td>Iran's foreign minister confirms that his coun...</td>\n",
       "      <td>Iran Says It Exceeded Enriched Uranium Cap\\nIr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.npr.org</td>\n",
       "      <td>https://www.npr.org/2019/07/01/737600999/iran-...</td>\n",
       "      <td>LEFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4037078</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Bombing Kills Dozens in Kabul as Taliban Talks...</td>\n",
       "      <td>Attackers set off bombs that wrecked a war mus...</td>\n",
       "      <td>KABUL, Afghanistan — A complex attack includin...</td>\n",
       "      <td>Thomas Gibbons-Neff;Rod Nordland</td>\n",
       "      <td>www.nytimes.com</td>\n",
       "      <td>https://www.nytimes.com/2019/07/01/world/asia/...</td>\n",
       "      <td>LEFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52904870</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>NPR</td>\n",
       "      <td>How Long Will The Current U.S. Economic Expans...</td>\n",
       "      <td>NPR's Steve Inskeep talks to David Wessel, dir...</td>\n",
       "      <td>How Long Will The Current U.S. Economic Expans...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.npr.org</td>\n",
       "      <td>https://www.npr.org/2019/07/01/737535414/how-l...</td>\n",
       "      <td>LEFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18311035</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>BBC</td>\n",
       "      <td>Black girls 'perceived as less innocent by US ...</td>\n",
       "      <td>New research from The Georgetown Law Center on...</td>\n",
       "      <td>Video\\nNew research from The Georgetown Law Ce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.bbc.com</td>\n",
       "      <td>http://www.bbc.com/news/av/world-us-canada-404...</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4061861</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Pete Buttigieg Raised $24.8 Million in Second ...</td>\n",
       "      <td>Mr. Buttigieg’s total is the latest evidence t...</td>\n",
       "      <td>WASHINGTON — Mayor Pete Buttigieg of South Ben...</td>\n",
       "      <td>Thomas Kaplan</td>\n",
       "      <td>www.nytimes.com</td>\n",
       "      <td>https://www.nytimes.com/2019/07/01/us/politics...</td>\n",
       "      <td>LEFT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id         date_publish              outlet  \\\n",
       "0  52972702  2019-07-01 00:00:00                 NPR   \n",
       "1   4037078  2019-07-01 00:00:00  The New York Times   \n",
       "2  52904870  2019-07-01 00:00:00                 NPR   \n",
       "3  18311035  2019-07-01 00:00:00                 BBC   \n",
       "4   4061861  2019-07-01 00:00:00  The New York Times   \n",
       "\n",
       "                                            headline  \\\n",
       "0         Iran Says It Exceeded Enriched Uranium Cap   \n",
       "1  Bombing Kills Dozens in Kabul as Taliban Talks...   \n",
       "2  How Long Will The Current U.S. Economic Expans...   \n",
       "3  Black girls 'perceived as less innocent by US ...   \n",
       "4  Pete Buttigieg Raised $24.8 Million in Second ...   \n",
       "\n",
       "                                                lead  \\\n",
       "0  Iran's foreign minister confirms that his coun...   \n",
       "1  Attackers set off bombs that wrecked a war mus...   \n",
       "2  NPR's Steve Inskeep talks to David Wessel, dir...   \n",
       "3  New research from The Georgetown Law Center on...   \n",
       "4  Mr. Buttigieg’s total is the latest evidence t...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Iran Says It Exceeded Enriched Uranium Cap\\nIr...   \n",
       "1  KABUL, Afghanistan — A complex attack includin...   \n",
       "2  How Long Will The Current U.S. Economic Expans...   \n",
       "3  Video\\nNew research from The Georgetown Law Ce...   \n",
       "4  WASHINGTON — Mayor Pete Buttigieg of South Ben...   \n",
       "\n",
       "                            authors           domain  \\\n",
       "0                               NaN      www.npr.org   \n",
       "1  Thomas Gibbons-Neff;Rod Nordland  www.nytimes.com   \n",
       "2                               NaN      www.npr.org   \n",
       "3                               NaN      www.bbc.com   \n",
       "4                     Thomas Kaplan  www.nytimes.com   \n",
       "\n",
       "                                                 url political_leaning  \n",
       "0  https://www.npr.org/2019/07/01/737600999/iran-...              LEFT  \n",
       "1  https://www.nytimes.com/2019/07/01/world/asia/...              LEFT  \n",
       "2  https://www.npr.org/2019/07/01/737535414/how-l...              LEFT  \n",
       "3  http://www.bbc.com/news/av/world-us-canada-404...         UNDEFINED  \n",
       "4  https://www.nytimes.com/2019/07/01/us/politics...              LEFT  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"./data/2019_2.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ALEJANDRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#lemmatization and removing stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    def is_english_word(word):\n",
    "        \"\"\"Function to filter out non-English words.\"\"\"\n",
    "        return bool(re.match(r'^[a-zA-Z]+$', word))\n",
    "    text = text.lower()\n",
    "    editorials_to_exclude = [\"cnn\", \"fox\", \"reuters\"]\n",
    "    for editorial in editorials_to_exclude:\n",
    "        text = re.sub(r\"\\b\" + re.escape(editorial) + r\"\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = contractions.fix(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [\n",
    "            lemmatizer.lemmatize(word) \n",
    "            for word in words \n",
    "            if word not in stop_words and is_english_word(word)\n",
    "        ]\n",
    "    words = [re.sub(r'[^\\w\\s]', '', token) for token in words if re.sub(r'[^\\w\\s]', '', token)]\n",
    "    words = [word for word in words if word]\n",
    "    return words[:5000]\n",
    "\n",
    "# Create Word2Vec Corpus for headline and body\n",
    "class MyCorpus:\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __iter__(self):\n",
    "        for text in self.texts:\n",
    "            yield preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = df['headline']#.apply(preprocess)\n",
    "body = df['body']#.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word2Vec model for headline and body separately\n",
    "headline_corpus = MyCorpus(headline)\n",
    "body_corpus = MyCorpus(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_model = Word2Vec(sentences=headline_corpus) #,vector_size=100, window=5, min_count=1, workers=4)\n",
    "body_model = Word2Vec(sentences=body_corpus) #,vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/7511 is trump\n",
      "word #1/7511 is say\n",
      "word #2/7511 is new\n",
      "word #3/7511 is police\n",
      "word #4/7511 is u\n",
      "word #5/7511 is china\n",
      "word #6/7511 is democrat\n",
      "word #7/7511 is shooting\n",
      "word #8/7511 is house\n",
      "word #9/7511 is johnson\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(headline_model.wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(headline_model.wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2367585   0.55916655  1.0645618   0.7336459  -1.3328441  -0.7641349\n",
      "  1.0223736   1.0448487  -1.1596005  -1.4593971  -0.02156663 -0.6333029\n",
      " -0.29212296  0.39364266 -0.06683157 -0.6610192  -0.18043107 -0.8112339\n",
      " -0.96861386 -1.0873138   0.56825334 -0.10102818  0.38629338 -0.11525298\n",
      "  0.91786367 -0.27058426 -0.6660524  -0.78718555 -0.42906767 -0.02413273\n",
      "  0.5271197   0.08535359  0.9772652  -0.40858665 -0.31021193 -0.43167362\n",
      "  0.08019218 -0.13841344  0.6650526  -1.6351038  -0.65190774  0.42510575\n",
      " -0.33795014 -1.0885165   0.40009487 -0.96452665 -0.64715     0.9959647\n",
      "  0.28871265  0.9041853  -0.23219971  0.28751174 -0.8142752  -0.12368076\n",
      "  1.4399587   0.04220992  0.62613404 -0.29919705 -0.0623568  -0.5649454\n",
      " -0.08017629  0.28489992  0.81683815 -0.37241918 -1.0314667   0.0226124\n",
      "  0.11279571  0.1579838  -1.7159396   0.33597183  0.04047381  0.5213772\n",
      " -0.8490265  -0.19223702  0.8210401   0.46775627  0.3600397   0.59452283\n",
      " -1.1771429  -0.3192185   0.38618296 -0.35849229  0.27775276  0.8007005\n",
      " -0.07251047  0.12302805  0.47655252 -0.73786205 -0.84652704  0.39353076\n",
      " -0.30565175 -0.04567049 -0.17553678  0.5979526   1.2241865   0.67286175\n",
      " -1.1374639  -0.51643634  0.36452276  0.93941486]\n",
      "[-0.2367585   0.55916655  1.0645618   0.7336459  -1.3328441  -0.7641349\n",
      "  1.0223736   1.0448487  -1.1596005  -1.4593971  -0.02156663 -0.6333029\n",
      " -0.29212296  0.39364266 -0.06683157 -0.6610192  -0.18043107 -0.8112339\n",
      " -0.96861386 -1.0873138   0.56825334 -0.10102818  0.38629338 -0.11525298\n",
      "  0.91786367 -0.27058426 -0.6660524  -0.78718555 -0.42906767 -0.02413273\n",
      "  0.5271197   0.08535359  0.9772652  -0.40858665 -0.31021193 -0.43167362\n",
      "  0.08019218 -0.13841344  0.6650526  -1.6351038  -0.65190774  0.42510575\n",
      " -0.33795014 -1.0885165   0.40009487 -0.96452665 -0.64715     0.9959647\n",
      "  0.28871265  0.9041853  -0.23219971  0.28751174 -0.8142752  -0.12368076\n",
      "  1.4399587   0.04220992  0.62613404 -0.29919705 -0.0623568  -0.5649454\n",
      " -0.08017629  0.28489992  0.81683815 -0.37241918 -1.0314667   0.0226124\n",
      "  0.11279571  0.1579838  -1.7159396   0.33597183  0.04047381  0.5213772\n",
      " -0.8490265  -0.19223702  0.8210401   0.46775627  0.3600397   0.59452283\n",
      " -1.1771429  -0.3192185   0.38618296 -0.35849229  0.27775276  0.8007005\n",
      " -0.07251047  0.12302805  0.47655252 -0.73786205 -0.84652704  0.39353076\n",
      " -0.30565175 -0.04567049 -0.17553678  0.5979526   1.2241865   0.67286175\n",
      " -1.1374639  -0.51643634  0.36452276  0.93941486]\n"
     ]
    }
   ],
   "source": [
    "vector_Trump = headline_model.wv['trump']\n",
    "print(vector_Trump)\n",
    "\n",
    "vector_Abortion = headline_model.wv['abortion']\n",
    "print(vector_Trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "headline_model.save('./models/headline_model')\n",
    "body_model.save('./models/body_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7511 - Embedding Dim: 100\n",
      "[('pelosi', 0.7955394983291626), ('lawmaker', 0.747930109500885), ('dems', 0.7463365197181702)]\n",
      "[('walsh', 0.9306197762489319), ('arpaio', 0.9298338890075684), ('kamala', 0.8947131633758545)]\n",
      "[('temporarily', 0.9445393681526184), ('allows', 0.9336333274841309), ('enforcement', 0.9268770217895508)]\n",
      "[('walsh', 0.9102801084518433), ('arpaio', 0.8938741683959961), ('scarborough', 0.8577998280525208)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "\n",
    "# Retrieve the weights from the model. This is used for initializing the weights\n",
    "# in a Keras Embedding layer later\n",
    "w2v_weights = headline_model.wv.vectors\n",
    "vocab_size, embedding_size = w2v_weights.shape\n",
    "\n",
    "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))\n",
    "\n",
    "# Some validation on the quality of the Word2Vec model\n",
    "print(headline_model.wv.most_similar('trump', topn=3))\n",
    "print(headline_model.wv.most_similar('biden', topn=3))\n",
    "print(headline_model.wv.most_similar('abortion', topn=3))\n",
    "print(headline_model.wv.most_similar(positive=['trump', 'biden'], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2token(word):\n",
    "    try:\n",
    "        return headline_model.wv.vocab[word].index\n",
    "    except KeyError:\n",
    "        return 0\n",
    "def token2word(token):\n",
    "    return headline_model.wv.index_to_key[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=2 must be between 1 and min(n_samples, n_features)=1 with svd_solver='randomized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Apply t-SNE to Word2Vec embeddings, reducing to 2 dims\u001b[39;00m\n\u001b[0;32m     11\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE()\n\u001b[1;32m---> 12\u001b[0m tsne_e \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_vecs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Plot t-SNE result\u001b[39;00m\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1176\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m-> 1176\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1027\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# Always output a numpy array, no matter what is configured globally\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m pca\u001b[38;5;241m.\u001b[39mset_output(transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1027\u001b[0m X_embedded \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;66;03m# PCA is rescaled so that PC1 has standard deviation 1e-4 which is\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;66;03m# the default value for random initialization. See issue #18018.\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m X_embedded \u001b[38;5;241m=\u001b[39m X_embedded \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(X_embedded[:, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:474\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:549\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_full(X, n_components, xp, is_array_api_compliant)\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_truncated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\7. DUKE\\1. ECE 684 - NLP\\Assignments\\Final Project\\venv_lda_implementation\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:724\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[1;34m(self, X, n_components, xp)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m cannot be a string with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;241m%\u001b[39m (n_components, svd_solver)\n\u001b[0;32m    722\u001b[0m     )\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m must be between 1 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    726\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    727\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    728\u001b[0m         \u001b[38;5;241m%\u001b[39m (n_components, \u001b[38;5;28mmin\u001b[39m(n_samples, n_features), svd_solver)\n\u001b[0;32m    729\u001b[0m     )\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m n_components \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    732\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m must be strictly less than \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;241m%\u001b[39m (n_components, \u001b[38;5;28mmin\u001b[39m(n_samples, n_features), svd_solver)\n\u001b[0;32m    736\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=2 must be between 1 and min(n_samples, n_features)=1 with svd_solver='randomized'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "n_samples = 500\n",
    "# Sample random words from model dictionary\n",
    "random_i = random.sample(range(vocab_size), n_samples)\n",
    "random_w = [token2word(i) for i in random_i]\n",
    "\n",
    "# Generate Word2Vec embeddings of each word\n",
    "word_vecs = np.array([[headline_model.wv.get_index(w)] for w in random_w])\n",
    "\n",
    "# Apply t-SNE to Word2Vec embeddings, reducing to 2 dims\n",
    "tsne = TSNE()\n",
    "tsne_e = tsne.fit_transform(word_vecs)\n",
    "\n",
    "# Plot t-SNE result\n",
    "plt.figure(figsize=(32, 32))\n",
    "plt.scatter(tsne_e[:, 0], tsne_e[:, 1], marker='o', c=range(len(random_w)), cmap=plt.get_cmap('Spectral'))\n",
    "\n",
    "for label, x, y, in zip(random_w, tsne_e[:, 0], tsne_e[:, 1]):\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y), xytext=(0, 15),\n",
    "                 textcoords='offset points', ha='right', va='bottom',\n",
    "                 bbox=dict(boxstyle='round, pad=0.2', fc='yellow', alpha=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/49643974/how-to-do-text-classification-using-word2vec\n",
    "\n",
    "https://stackoverflow.com/questions/57525190/text-classification-with-word2vec\n",
    "\n",
    "https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.Similarity\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "\n",
    "https://www.kaggle.com/code/guichristmann/lstm-classification-model-with-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Convert text to sequences of Word2Vec embeddings\n",
    "def get_word2vec_sequence(text, model):\n",
    "    words = preprocess(text)\n",
    "    word_vectors = [torch.tensor(model.wv[word], dtype=torch.float32) for word in words if word in model.wv]\n",
    "    return torch.stack(word_vectors) if word_vectors else torch.zeros((1, model.vector_size))\n",
    "\n",
    "# Convert the corpus into padded sequences\n",
    "def get_padded_sequences(corpus, model, max_length=500):\n",
    "    sequences = [get_word2Svec_sequence(text, model) for text in corpus]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)  # Pad to the longest sequence\n",
    "    return padded_sequences[:, :max_length]  # Optional: Truncate to `max_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliticalLeaningDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences\n",
    "headline_sequences = get_padded_sequences(headline_corpus, headline_model)\n",
    "body_sequences = get_padded_sequences(body_corpus, body_model)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "political_leaning_encoded = label_encoder.fit_transform(df['political_leaning'])\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    headline_sequences, political_leaning_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = PoliticalLeaningDataset(X_train, y_train)\n",
    "test_dataset = PoliticalLeaningDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader instances for batching\n",
    "train_dataset = PoliticalLeaningDataset(X_train, y_train)\n",
    "test_dataset = PoliticalLeaningDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class PoliticalLeaningModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PoliticalLeaningModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, output_dim)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_dim = embeddings.shape[1]  # This is the size of the feature vector (headline + body)\n",
    "output_dim = len(label_encoder.classes_)  # Number of political leaning classes\n",
    "model = PoliticalLeaningModel(input_dim, output_dim)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict political leaning on new data\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor(data, dtype=torch.float32))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.numpy()\n",
    "\n",
    "# Predict political leaning for test data\n",
    "predicted_labels = predict(model, X_test)\n",
    "\n",
    "# Decode the predicted labels back to original political leaning classes\n",
    "predicted_political_leaning = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Print first 10 predictions\n",
    "for i in range(10):\n",
    "    print(f\"Predicted: {predicted_political_leaning[i]}, Actual: {label_encoder.inverse_transform([y_test[i]])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre trained word2vec**\n",
    "\n",
    "https://radimrehurek.com/gensim/similarities/docsim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### document2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    TaggedDocument(\"I love machine learning\", [0]),\n",
    "    TaggedDocument(\"Machine learning is the future\", [1]),\n",
    "    TaggedDocument(\"Deep learning is a subset of machine learning\", [2])\n",
    "]\n",
    "\n",
    "# Train Doc2Vec model\n",
    "model = Doc2Vec(documents, vector_size=100, window=5, min_count=2, epochs=40)\n",
    "\n",
    "# Infer vector for a new document\n",
    "new_document = \"I enjoy natural language processing\"\n",
    "new_vector = model.infer_vector(new_document.split())\n",
    "\n",
    "# Find most similar documents\n",
    "similar_docs = model.dv.most_similar([new_vector])\n",
    "\n",
    "print(similar_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_lda_implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
